#!/usr/bin/env python3
"""
æ›¸é¡åˆ†é¡ã‚¨ãƒ³ã‚¸ãƒ³ v5.0
ANDæ¡ä»¶å¯¾å¿œãƒ»é«˜ç²¾åº¦æ›¸é¡ç¨®åˆ¥åˆ¤å®šã‚·ã‚¹ãƒ†ãƒ ï¼ˆå®Œå…¨æ”¹è¨‚ç‰ˆï¼‰
"""

import re
import logging
import json
import os
from typing import Dict, List, Optional, Tuple, Callable, Union, Any
from dataclasses import dataclass, field
import datetime
from pathlib import Path

@dataclass
class AndCondition:
    """ANDæ¡ä»¶ã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    keywords: List[str]
    match_type: str = "all"  # "all" (ã™ã¹ã¦å¿…è¦) or "any" (ã„ãšã‚Œã‹å¿…è¦)
    
    def check_match(self, text: str) -> Tuple[bool, List[str]]:
        """ANDæ¡ä»¶ã®ãƒãƒƒãƒãƒ³ã‚°ãƒã‚§ãƒƒã‚¯"""
        matched = []
        for keyword in self.keywords:
            if keyword in text:
                matched.append(keyword)
        
        if self.match_type == "all":
            return len(matched) == len(self.keywords), matched
        else:  # "any"
            return len(matched) > 0, matched

@dataclass
class ClassificationStep:
    """åˆ†é¡ã®1ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    document_type: str
    score: float
    matched_keywords: List[str]
    excluded: bool
    exclude_reason: str = ""
    method: str = "normal"  # "highest_priority", "and_condition", "normal"

@dataclass
class ClassificationResult:
    """åˆ†é¡çµæœã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    document_type: str
    confidence: float
    matched_keywords: List[str]
    classification_method: str
    debug_steps: List[ClassificationStep] = field(default_factory=list)
    processing_log: List[str] = field(default_factory=list)
    original_doc_type_code: Optional[str] = None  # å…ƒã®åˆ†é¡ã‚³ãƒ¼ãƒ‰ï¼ˆè‡ªæ²»ä½“é©ç”¨å‰ï¼‰
    meta: Dict[str, Any] = field(default_factory=dict)  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆno_splitç­‰ï¼‰
    prefecture_code: Optional[int] = None  # éƒ½é“åºœçœŒé€£ç•ªã‚³ãƒ¼ãƒ‰ï¼ˆ1001/1011/1021ç­‰ï¼‰
    city_code: Optional[int] = None  # å¸‚åŒºç”ºæ‘é€£ç•ªã‚³ãƒ¼ãƒ‰ï¼ˆ2001/2011/2021ç­‰ï¼‰

class DocumentClassifierV5:
    """æ›¸é¡åˆ†é¡ã‚¨ãƒ³ã‚¸ãƒ³ v5.0 - ANDæ¡ä»¶å¯¾å¿œç‰ˆ"""
    
    def __init__(self, debug_mode: bool = False, log_callback: Optional[Callable[[str], None]] = None):
        """åˆæœŸåŒ–
        
        Args:
            debug_mode: ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®æœ‰åŠ¹åŒ–
            log_callback: ãƒ­ã‚°å‡ºåŠ›ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°
        """
        self.debug_mode = debug_mode
        self.log_callback = log_callback
        self.current_filename = ""
        self.processing_log = []
        
        # v5.0 æ–°åˆ†é¡ãƒ«ãƒ¼ãƒ«ï¼ˆANDæ¡ä»¶å¯¾å¿œï¼‰
        self.classification_rules_v5 = self._initialize_classification_rules_v5()
        
        # v5.3.4 prefecture code mapping for local tax
        self.prefecture_code_map = {
            "æ±äº¬éƒ½": "1001",
            "æ„›çŸ¥çœŒ": "1011", 
            "ç¦å²¡çœŒ": "1021",
            "å¤§é˜ªåºœ": "1031",
            "ç¥å¥ˆå·çœŒ": "1041"
        }

    def code_domain(self, code: str) -> str:
        """ã‚³ãƒ¼ãƒ‰ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¤å®š - ãƒã‚¤ã‚ºæŠ‘åˆ¶ã®ãŸã‚ã®é–€ç•ª"""
        if not code or not isinstance(code, str):
            return "UNKNOWN"
        
        # ã‚³ãƒ¼ãƒ‰ã®å…ˆé ­æ•°å­—ã§ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’åˆ¤å®š
        first_digit = code[0] if code else ""
        
        domain_map = {
            "0": "NATIONAL_TAX",      # å›½ç¨
            "1": "LOCAL_TAX",         # åœ°æ–¹ç¨ï¼ˆéƒ½é“åºœçœŒï¼‰
            "2": "LOCAL_TAX",         # åœ°æ–¹ç¨ï¼ˆå¸‚ç”ºæ‘ï¼‰
            "3": "CONSUMPTION_TAX",   # æ¶ˆè²»ç¨
            "5": "ACCOUNTING",        # ä¼šè¨ˆæ›¸é¡
            "6": "ASSETS",           # è³‡ç”£
            "7": "SUMMARY"           # é›†è¨ˆãƒ»ãã®ä»–
        }
        
        return domain_map.get(first_digit, "UNKNOWN")

    def resolve_local_tax_class(self, base_class: str, prefecture: Optional[str] = None, 
                               city: Optional[str] = None) -> str:
        """LOCAL_TAX ãƒ‰ãƒ¡ã‚¤ãƒ³ã®å ´åˆã®ã¿ã€è‡ªæ²»ä½“åˆ¥ã®æœ€çµ‚ã‚¯ãƒ©ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºå®š"""
        if not base_class:
            return base_class
            
        # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯ - LOCAL_TAXä»¥å¤–ã¯ãã®ã¾ã¾è¿”ã™
        if self.code_domain(base_class) != "LOCAL_TAX":
            return base_class
        
        # éƒ½é“åºœçœŒç¨ã®å ´åˆï¼ˆ1001ç³»ï¼‰
        if base_class.startswith("1001") or "éƒ½é“åºœçœŒ" in base_class:
            if prefecture and prefecture in self.prefecture_code_map:
                upgraded_code = self.prefecture_code_map[prefecture]
                # å…ƒã®å½¢å¼ã‚’ä¿æŒã—ãªãŒã‚‰ã‚³ãƒ¼ãƒ‰ã ã‘ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰
                if "_" in base_class:
                    parts = base_class.split("_", 1)
                    return f"{upgraded_code}_{parts[1]}"
                else:
                    return upgraded_code
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: base_class ã‚’ãã®ã¾ã¾è¿”ã™
            return base_class
        
        # å¸‚æ°‘ç¨ã®å ´åˆï¼ˆ2001ç³»ï¼‰- å¾“æ¥é€šã‚Š
        if base_class.startswith("2001") or "å¸‚æ°‘ç¨" in base_class:
            return base_class
        
        # ãã®ä»–ã®åœ°æ–¹ç¨ã‚³ãƒ¼ãƒ‰
        return base_class

    def _initialize_classification_rules_v5(self) -> Dict:
        """v5.0 åˆ†é¡ãƒ«ãƒ¼ãƒ«åˆæœŸåŒ–ï¼ˆANDæ¡ä»¶å¯¾å¿œï¼‰"""
        return {
            # ===== 0000ç•ªå° - å›½ç¨ç”³å‘Šæ›¸é¡ =====
            "0000_ç´ä»˜ç¨é¡ä¸€è¦§è¡¨": {
                "priority": 150,  # é©åº¦ãªå„ªå…ˆåº¦
                "highest_priority_conditions": [
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã«ã‚ˆã‚‹ç¢ºå®Ÿãªåˆ¤å®šã®ã¿ï¼ˆéåº¦ãªæ¡ä»¶ã‚’å‰Šé™¤ï¼‰
                    AndCondition(["ç´ç¨ä¸€è¦§"], "any"),
                    AndCondition(["ç¨é¡ä¸€è¦§è¡¨"], "any")
                ],
                "exact_keywords": [
                    "ç´ä»˜ç¨é¡ä¸€è¦§è¡¨", "ç´ç¨ä¸€è¦§", "ç¨é¡ä¸€è¦§è¡¨"
                ],
                "partial_keywords": [
                    "ç¨é¡ä¸€è¦§"
                ],
                "exclude_keywords": [
                    # ä»–ã®æ›¸é¡ã‚’æ˜ç¢ºã«é™¤å¤–ï¼ˆéåº¦ãªé©ç”¨ã‚’é˜²æ­¢ï¼‰
                    "ç”³å‘Šæ›¸", "ç¢ºå®šç”³å‘Š", "é’è‰²ç”³å‘Š", "å†…å›½æ³•äººã®ç¢ºå®šç”³å‘Š",
                    "æ±ºç®—æ›¸", "è²¸å€Ÿå¯¾ç…§è¡¨", "æç›Šè¨ˆç®—æ›¸",
                    "ä»•è¨³å¸³", "ç·å‹˜å®šå…ƒå¸³", "è£œåŠ©å…ƒå¸³", "æ®‹é«˜è©¦ç®—è¡¨",
                    "å—ä¿¡é€šçŸ¥", "ç´ä»˜æƒ…å ±ç™ºè¡Œçµæœ", "ç´ä»˜åŒºåˆ†ç•ªå·é€šçŸ¥", "ãƒ¡ãƒ¼ãƒ«è©³ç´°",
                    "çœŒç¨äº‹å‹™æ‰€", "éƒ½ç¨äº‹å‹™æ‰€", "å¸‚å½¹æ‰€",
                    "æ³•äººéƒ½é“åºœçœŒæ°‘ç¨", "å¸‚ç”ºæ‘ç”³å‘Šæ›¸", "æ¶ˆè²»ç¨ç”³å‘Šæ›¸",
                    "ä¸€æ‹¬å„Ÿå´", "å°‘é¡æ¸›ä¾¡å„Ÿå´", "å›ºå®šè³‡ç”£å°å¸³", "å‹˜å®šç§‘ç›®åˆ¥"
                ],
                "filename_keywords": ["ç´ç¨ä¸€è¦§", "ç¨é¡ä¸€è¦§"]
            },
            
            "0001_æ³•äººç¨ç­‰ç”³å‘Šæ›¸": {
                "priority": 200,  # æœ€é«˜å„ªå…ˆåº¦ã«å¤‰æ›´
                "highest_priority_conditions": [
                    # ä¿®æ­£æŒ‡ç¤ºæ›¸ã«åŸºã¥ãæ–°ã—ã„æœ€å„ªå…ˆæ¡ä»¶ã‚’è¿½åŠ 
                    AndCondition(["01_å†…å›½æ³•äºº", "ç¢ºå®šç”³å‘Š"], "all"),  # ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³
                    AndCondition(["äº‹æ¥­å¹´åº¦åˆ†ã®æ³•äººç¨ç”³å‘Šæ›¸", "å·®å¼•ç¢ºå®šæ³•äººç¨é¡"], "all"),
                    AndCondition(["æ§é™¤ã—ãã‚Œãªã‹ã£ãŸé‡‘é¡", "èª²ç¨ç•™ä¿é‡‘é¡"], "all"),
                    AndCondition(["ä¸­é–“ç”³å‘Šåˆ†ã®æ³•äººç¨é¡", "ä¸­é–“ç”³å‘Šåˆ†ã®åœ°æ–¹æ³•äººç¨é¡"], "all")
                ],
                "exact_keywords": [
                    "æ³•äººç¨åŠã³åœ°æ–¹æ³•äººç¨ç”³å‘Šæ›¸",
                    "æ³•äººç¨ç”³å‘Šæ›¸åˆ¥è¡¨ä¸€", "ç”³å‘Šæ›¸ç¬¬ä¸€è¡¨"
                ],
                "partial_keywords": ["æ³•äººç¨ç”³å‘Š", "å†…å›½æ³•äºº", "ç¢ºå®šç”³å‘Š", "é’è‰²ç”³å‘Š"],
                "exclude_keywords": ["ãƒ¡ãƒ¼ãƒ«è©³ç´°", "å—ä¿¡é€šçŸ¥", "ç´ä»˜åŒºåˆ†ç•ªå·é€šçŸ¥", "æ·»ä»˜è³‡æ–™", "ã‚¤ãƒ¡ãƒ¼ã‚¸æ·»ä»˜"],
                "filename_keywords": ["å†…å›½æ³•äºº", "ç¢ºå®šç”³å‘Š", "é’è‰²"]
            },
            
            "0002_æ·»ä»˜è³‡æ–™_æ³•äººç¨": {
                "priority": 200,  # ãƒã‚°ä¿®æ­£ä¾é ¼æ›¸: B-2 æœ€é«˜å„ªå…ˆåº¦ã«å¤‰æ›´
                "highest_priority_conditions": [
                    # æ–°ã—ã„æœ€å„ªå…ˆæ¡ä»¶: æ·»ä»˜æ›¸é¡é€ä»˜æ›¸ã€æ·»ä»˜æ›¸é¡åç§°ã€å†…å›½æ³•äººã®ç¢ºå®šç”³å‘Šã®3ã¤ã§ç¢ºå®Ÿã«åˆ¤å®š
                    AndCondition(["æ·»ä»˜æ›¸é¡é€ä»˜æ›¸", "æ·»ä»˜æ›¸é¡åç§°", "å†…å›½æ³•äººã®ç¢ºå®šç”³å‘Š"], "all"),
                    AndCondition(["æ·»ä»˜æ›¸é¡åç§°"], "any"), 
                    # ãƒã‚°ä¿®æ­£ä¾é ¼æ›¸: B-1 å®Œå…¨ä¸€è‡´ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®è¿½åŠ 
                    AndCondition(["æ³•äººç¨ æ·»ä»˜è³‡æ–™"], "any"),
                    AndCondition(["æ·»ä»˜è³‡æ–™ æ³•äººç¨"], "any"), 
                    AndCondition(["ã‚¤ãƒ¡ãƒ¼ã‚¸æ·»ä»˜æ›¸é¡(æ³•äººç¨ç”³å‘Š)"], "any"),
                    AndCondition(["ã‚¤ãƒ¡ãƒ¼ã‚¸æ·»ä»˜æ›¸é¡ æ³•äººç¨"], "any"),
                    AndCondition(["æ·»ä»˜æ›¸é¡ æ³•äººç¨"], "any"),
                    # æ—¢å­˜æ¡ä»¶ã‚‚ç¶­æŒ
                    AndCondition(["æ·»ä»˜è³‡æ–™", "æ³•äººç¨ç”³å‘Š", "ã‚¤ãƒ¡ãƒ¼ã‚¸æ·»ä»˜"], "all"),
                    AndCondition(["æ·»ä»˜æ›¸é¡", "æ³•äººç¨", "ç”³å‘Šæ›¸"], "all")
                ],
                "exact_keywords": [
                    "æ·»ä»˜æ›¸é¡é€ä»˜æ›¸", "æ·»ä»˜æ›¸é¡åç§°", "å†…å›½æ³•äººã®ç¢ºå®šç”³å‘Š",
                    "æ³•äººç¨ æ·»ä»˜è³‡æ–™", "æ·»ä»˜è³‡æ–™ æ³•äººç¨", "ã‚¤ãƒ¡ãƒ¼ã‚¸æ·»ä»˜æ›¸é¡(æ³•äººç¨ç”³å‘Š)",
                    "ã‚¤ãƒ¡ãƒ¼ã‚¸æ·»ä»˜æ›¸é¡ æ³•äººç¨", "æ·»ä»˜æ›¸é¡ æ³•äººç¨"
                ],
                "partial_keywords": ["æ·»ä»˜è³‡æ–™", "æ³•äººç¨ è³‡æ–™", "ã‚¤ãƒ¡ãƒ¼ã‚¸æ·»ä»˜", "æ·»ä»˜æ›¸é¡"],
                "exclude_keywords": ["æ¶ˆè²»ç¨ç”³å‘Š", "æ³•äººæ¶ˆè²»ç¨", "æ¶ˆè²»ç¨", "å—ä¿¡é€šçŸ¥", "ç´ä»˜åŒºåˆ†ç•ªå·é€šçŸ¥"],
                "filename_keywords": ["æ³•äººç¨ç”³å‘Š", "æ³•äººç¨", "å†…å›½æ³•äºº"]
            },
            
            "0003_å—ä¿¡é€šçŸ¥": {
                "priority": 130,
                "highest_priority_conditions": [
                    AndCondition(["ãƒ¡ãƒ¼ãƒ«è©³ç´°", "ç¨®ç›® æ³•äººç¨åŠã³åœ°æ–¹æ³•äººç¨ç”³å‘Šæ›¸"], "all"),
                    AndCondition(["å—ä»˜ç•ªå·", "ç¨ç›® æ³•äººç¨", "å—ä»˜æ—¥æ™‚"], "all"),
                    AndCondition(["æå‡ºå…ˆ", "ç¨å‹™ç½²", "æ³•äººç¨åŠã³åœ°æ–¹æ³•äººç¨ç”³å‘Šæ›¸"], "all"),
                    AndCondition(["é€ä¿¡ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘ä»˜ã‘ã¾ã—ãŸ", "æ³•äººç¨"], "all")
                ],
                "exact_keywords": ["æ³•äººç¨ å—ä¿¡é€šçŸ¥", "å—ä¿¡é€šçŸ¥ æ³•äººç¨"],
                "partial_keywords": ["å—ä¿¡é€šçŸ¥", "å›½ç¨é›»å­ç”³å‘Š", "ãƒ¡ãƒ¼ãƒ«è©³ç´°"],
                "exclude_keywords": ["æ¶ˆè²»ç¨ç”³å‘Šæ›¸", "ç´ä»˜åŒºåˆ†ç•ªå·é€šçŸ¥"],
                "filename_keywords": ["å—ä¿¡é€šçŸ¥", "æ³•äººç¨"]
            },
            
            "0004_ç´ä»˜æƒ…å ±": {
                "priority": 130,
                "highest_priority_conditions": [
                    AndCondition(["ãƒ¡ãƒ¼ãƒ«è©³ç´°ï¼ˆç´ä»˜åŒºåˆ†ç•ªå·é€šçŸ¥ï¼‰", "æ³•äººç¨åŠåœ°æ–¹æ³•äººç¨"], "all"),
                    AndCondition(["ç´ä»˜åŒºåˆ†ç•ªå·é€šçŸ¥", "ç¨ç›® æ³•äººç¨åŠåœ°æ–¹æ³•äººç¨"], "all"),
                    AndCondition(["ç´ä»˜å…ˆ", "ç¨å‹™ç½²", "æ³•äººç¨åŠåœ°æ–¹æ³•äººç¨"], "all"),
                    AndCondition(["ç´ä»˜å†…å®¹ã‚’ç¢ºèªã—", "æ³•äººç¨"], "all")
                ],
                "exact_keywords": ["æ³•äººç¨ ç´ä»˜æƒ…å ±", "ç´ä»˜æƒ…å ± æ³•äººç¨", "ç´ä»˜åŒºåˆ†ç•ªå·é€šçŸ¥"],
                "partial_keywords": ["ç´ä»˜æƒ…å ±", "ç´ä»˜æ›¸", "å›½ç¨ ç´ä»˜"],
                "exclude_keywords": ["æ¶ˆè²»ç¨åŠåœ°æ–¹æ¶ˆè²»ç¨", "å—ä¿¡é€šçŸ¥"],
                "filename_keywords": ["ç´ä»˜æƒ…å ±", "æ³•äººç¨"]
            },
            
            # ===== 1000ç•ªå° - éƒ½é“åºœçœŒç¨é–¢é€£ =====
            "1001_éƒ½é“åºœçœŒ_éƒ½é“åºœçœŒç”³å‘Šæ›¸": {
                "priority": 135,
                "highest_priority_conditions": [
                    AndCondition(["éƒ½é“åºœçœŒç”³å‘Šæ›¸ç”³å‘Šæ›¸", "å¹´400ä¸‡å††ä»¥ä¸‹"], "all"),
                    AndCondition(["çœŒç¨äº‹å‹™æ‰€", "æ³•äººäº‹æ¥­ç¨", "ç‰¹åˆ¥æ³•äººäº‹æ¥­ç¨"], "all"),
                    AndCondition(["éƒ½ç¨äº‹å‹™æ‰€", "é“åºœçœŒæ°‘ç¨", "äº‹æ¥­ç¨"], "all"),
                    AndCondition(["æ³•äººäº‹æ¥­ç¨ç”³å‘Šæ›¸", "éƒ½é“åºœçœŒæ°‘ç¨"], "all")
                ],
                "exact_keywords": [
                    "éƒ½é“åºœçœŒç”³å‘Šæ›¸ç”³å‘Šæ›¸", "æ³•äººäº‹æ¥­ç¨ç”³å‘Šæ›¸", "éƒ½é“åºœçœŒæ°‘ç¨ç”³å‘Šæ›¸"
                ],
                "partial_keywords": [
                    "éƒ½é“åºœçœŒæ°‘ç¨", "æ³•äººäº‹æ¥­ç¨", "ç‰¹åˆ¥æ³•äººäº‹æ¥­ç¨", "é“åºœçœŒæ°‘ç¨", "äº‹æ¥­ç¨",
                    "çœŒç¨äº‹å‹™æ‰€", "éƒ½ç¨äº‹å‹™æ‰€", "å¹´400ä¸‡å††ä»¥ä¸‹", "å¹´æœˆæ—¥ã‹ã‚‰å¹´æœˆæ—¥ã¾ã§ã®"
                ],
                "exclude_keywords": ["å¸‚ç”ºæ‘", "å¸‚æ°‘ç¨", "å¸‚å½¹æ‰€", "ç”ºå½¹å ´", "æ‘å½¹å ´", "å—ä¿¡é€šçŸ¥", "ç´ä»˜æƒ…å ±"],
                "filename_keywords": ["çœŒç¨äº‹å‹™æ‰€", "éƒ½ç¨äº‹å‹™æ‰€"]
            },
            
            "1003_å—ä¿¡é€šçŸ¥": {
                "priority": 130,
                "highest_priority_conditions": [
                    AndCondition(["ç”³å‘Šå—ä»˜å®Œäº†é€šçŸ¥", "éƒ½é“åºœçœŒæ°‘ç¨", "äº‹æ¥­ç¨"], "all"),
                    AndCondition(["çœŒç¨äº‹å‹™æ‰€", "å—ä¿¡é€šçŸ¥", "æ³•äººäº‹æ¥­ç¨"], "all"),
                    AndCondition(["éƒ½ç¨äº‹å‹™æ‰€", "å—ä»˜å®Œäº†é€šçŸ¥", "ç‰¹åˆ¥æ³•äººäº‹æ¥­ç¨"], "all")
                ],
                "exact_keywords": ["éƒ½é“åºœçœŒ å—ä¿¡é€šçŸ¥"],
                "partial_keywords": ["å—ä¿¡é€šçŸ¥", "åœ°æ–¹ç¨é›»å­ç”³å‘Š"],
                "exclude_keywords": ["å¸‚ç”ºæ‘", "å¸‚æ°‘ç¨", "å›½ç¨é›»å­ç”³å‘Š"],
                "filename_keywords": ["å—ä¿¡é€šçŸ¥", "éƒ½é“åºœçœŒ"]
            },
            
            "1004_ç´ä»˜æƒ…å ±": {
                "priority": 130,
                "highest_priority_conditions": [
                    AndCondition(["ç´ä»˜æƒ…å ±ç™ºè¡Œçµæœ", "æ³•äººäºŒç¨ãƒ»ç‰¹åˆ¥ç¨"], "all"),
                    AndCondition(["åœ°æ–¹ç¨å…±åŒæ©Ÿæ§‹", "æ³•äººéƒ½é“åºœçœŒæ°‘ç¨ãƒ»äº‹æ¥­ç¨"], "all"),
                    AndCondition(["ç¨ç›®:æ³•äººäºŒç¨ãƒ»ç‰¹åˆ¥ç¨", "ç´ä»˜æƒ…å ±ãŒç™ºè¡Œã•ã‚Œ"], "all"),
                    AndCondition(["ãƒšã‚¤ã‚¸ãƒ¼ç´ä»˜æƒ…å ±", "éƒ½é“åºœçœŒæ°‘ç¨"], "all")
                ],
                "exact_keywords": ["éƒ½é“åºœçœŒ ç´ä»˜æƒ…å ±", "ç´ä»˜æƒ…å ±ç™ºè¡Œçµæœ", "åœ°æ–¹ç¨å…±åŒæ©Ÿæ§‹"],
                "partial_keywords": ["ç´ä»˜æƒ…å ±", "åœ°æ–¹ç¨ ç´ä»˜", "æ³•äººäºŒç¨", "ç‰¹åˆ¥ç¨"],
                "exclude_keywords": ["å¸‚å½¹æ‰€", "ç”ºå½¹å ´", "æ‘å½¹å ´", "å¸‚ç”ºæ‘ç”³å‘Šæ›¸", "å›½ç¨"],
                "filename_keywords": ["ç´ä»˜æƒ…å ±", "éƒ½é“åºœçœŒ"]
            },
            
            # ===== 2000ç•ªå° - å¸‚ç”ºæ‘ç¨é–¢é€£ =====
            "2001_å¸‚ç”ºæ‘_å¸‚ç”ºæ‘ç”³å‘Šæ›¸": {
                "priority": 180,  # ãƒã‚°ä¿®æ­£ä¾é ¼æ›¸: C-2 å„ªå…ˆåº¦ã‚’é«˜ãè¨­å®š
                "highest_priority_conditions": [
                    # æŒ‡å®šã•ã‚ŒãŸ2ã¤ã®ANDæ¡ä»¶ã®ã¿ï¼ˆæœ€å„ªå…ˆï¼‰
                    AndCondition(["å½“è©²å¸‚ç”ºæ‘å†…ã«æ‰€åœ¨"], "any"),
                    AndCondition(["å¸‚ç”ºæ‘æ°‘ç¨ã®ç‰¹å®šå¯„é™„é‡‘"], "any")
                ],
                "exact_keywords": ["å¸‚ç”ºæ‘ç”³å‘Šæ›¸ç”³å‘Šæ›¸", "å¸‚æ°‘ç¨ç”³å‘Šæ›¸"],
                "partial_keywords": ["å¸‚ç”ºæ‘ç”³å‘Šæ›¸", "å¸‚ç”ºæ‘æ°‘ç¨", "å¸‚å½¹æ‰€", "ç”ºå½¹å ´", "æ‘å½¹å ´"],
                "exclude_keywords": [
                    "éƒ½é“åºœçœŒ", "äº‹æ¥­ç¨", "çœŒç¨äº‹å‹™æ‰€", "éƒ½ç¨äº‹å‹™æ‰€", "å—ä¿¡é€šçŸ¥", "ç´ä»˜æƒ…å ±",
                    # ãƒã‚°ä¿®æ­£ä¾é ¼æ›¸: C-1 é™¤å¤–æ¡ä»¶ã®è¿½åŠ 
                    "å†…å›½æ³•äºº", "ç¢ºå®šç”³å‘Š(é’è‰²)", "äº‹æ¥­å¹´åº¦åˆ†", "ç¨é¡æ§é™¤"
                ],
                "filename_keywords": ["å¸‚å½¹æ‰€", "å¸‚æ°‘ç¨"]
            },
            
            "2003_å—ä¿¡é€šçŸ¥": {
                "priority": 140,
                "highest_priority_conditions": [
                    AndCondition(["ç”³å‘Šå—ä»˜å®Œäº†é€šçŸ¥", "æ³•äººå¸‚ç”ºæ‘æ°‘ç¨"], "all"),
                    AndCondition(["ç”³å‘Šå—ä»˜å®Œäº†é€šçŸ¥", "å¸‚ç”ºæ‘ç”³å‘Šæ›¸"], "all"),
                    AndCondition(["å¸‚ç”ºæ‘ç”³å‘Šæ›¸", "å¸‚å½¹æ‰€", "ç”³å‘Šå—ä»˜å®Œäº†é€šçŸ¥"], "all"),
                    AndCondition(["å¸‚é•·", "å¸‚ç”ºæ‘ç”³å‘Šæ›¸", "å—ä»˜å®Œäº†é€šçŸ¥"], "all"),
                    # å…·ä½“çš„ãªå¸‚ç”ºæ‘åã§ã®åˆ¤å®š
                    AndCondition(["è’²éƒ¡å¸‚å½¹æ‰€", "ç”³å‘Šå—ä»˜å®Œäº†é€šçŸ¥"], "all"),
                    AndCondition(["ç¦å²¡å¸‚", "å¸‚ç”ºæ‘ç”³å‘Šæ›¸", "å—ä»˜ç•ªå·"], "all")
                ],
                "exact_keywords": ["å¸‚ç”ºæ‘ å—ä¿¡é€šçŸ¥", "ç”³å‘Šå—ä»˜å®Œäº†é€šçŸ¥"],
                "partial_keywords": ["å—ä¿¡é€šçŸ¥", "åœ°æ–¹ç¨é›»å­ç”³å‘Š", "å¸‚å½¹æ‰€"],
                "exclude_keywords": ["çœŒç¨äº‹å‹™æ‰€", "éƒ½ç¨äº‹å‹™æ‰€", "æ³•äººäº‹æ¥­ç¨", "å›½ç¨é›»å­ç”³å‘Š"],
                "filename_keywords": ["å—ä¿¡é€šçŸ¥", "å¸‚ç”ºæ‘"]
            },
            
            "2004_ç´ä»˜æƒ…å ±": {
                "priority": 130,
                "highest_priority_conditions": [
                    AndCondition(["ç´ä»˜æƒ…å ±ç™ºè¡Œçµæœ", "æ³•äººä½æ°‘ç¨"], "all"),
                    AndCondition(["å¸‚å½¹æ‰€", "ç´ä»˜æƒ…å ±", "å¸‚ç”ºæ‘ç”³å‘Šæ›¸"], "all"),
                    AndCondition(["åœ°æ–¹ç¨å…±åŒæ©Ÿæ§‹", "æ³•äººå¸‚ç”ºæ‘æ°‘ç¨"], "all")
                ],
                "exact_keywords": ["å¸‚ç”ºæ‘ ç´ä»˜æƒ…å ±", "æ³•äººä½æ°‘ç¨ ç´ä»˜æƒ…å ±"],
                "partial_keywords": ["ç´ä»˜æƒ…å ±", "åœ°æ–¹ç¨ ç´ä»˜", "æ³•äººä½æ°‘ç¨"],
                "exclude_keywords": ["çœŒç¨äº‹å‹™æ‰€", "éƒ½ç¨äº‹å‹™æ‰€", "æ³•äººäºŒç¨ãƒ»ç‰¹åˆ¥ç¨", "å›½ç¨"],
                "filename_keywords": ["ç´ä»˜æƒ…å ±", "å¸‚ç”ºæ‘"]
            },
            
            # ===== 3000ç•ªå° - æ¶ˆè²»ç¨é–¢é€£ =====
            "3001_æ¶ˆè²»ç¨ç­‰ç”³å‘Šæ›¸": {
                "priority": 135,
                "highest_priority_conditions": [
                    AndCondition(["èª²ç¨æœŸé–“åˆ†ã®æ¶ˆè²»ç¨åŠã³", "åŸºæº–æœŸé–“ã®"], "all"),
                    AndCondition(["æ¶ˆè²»ç¨åŠã³åœ°æ–¹æ¶ˆè²»ç¨ç”³å‘Š(ä¸€èˆ¬ãƒ»æ³•äºº)", "èª²ç¨æ¨™æº–é¡"], "all"),
                    AndCondition(["ç¾é‡‘ä¸»ç¾©ä¼šè¨ˆã®é©ç”¨", "æ¶ˆè²»ç¨ç”³å‘Š"], "all"),
                    AndCondition(["èª²ç¨æ¨™æº–é¡", "æ¶ˆè²»ç¨åŠã³åœ°æ–¹æ¶ˆè²»ç¨ã®åˆè¨ˆç¨é¡"], "all")
                ],
                "exact_keywords": [
                    "æ¶ˆè²»ç¨ç”³å‘Šæ›¸", "æ¶ˆè²»ç¨åŠã³åœ°æ–¹æ¶ˆè²»ç¨ç”³å‘Šæ›¸",
                    "æ¶ˆè²»ç¨åŠã³åœ°æ–¹æ¶ˆè²»ç¨ç”³å‘Š(ä¸€èˆ¬ãƒ»æ³•äºº)", "æ¶ˆè²»ç¨ç”³å‘Š(ä¸€èˆ¬ãƒ»æ³•äºº)",
                    "èª²ç¨æœŸé–“åˆ†ã®æ¶ˆè²»ç¨åŠã³", "åŸºæº–æœŸé–“ã®", "ç¾é‡‘ä¸»ç¾©ä¼šè¨ˆã®é©ç”¨"
                ],
                "partial_keywords": ["æ¶ˆè²»ç¨ç”³å‘Š", "åœ°æ–¹æ¶ˆè²»ç¨ç”³å‘Š", "æ¶ˆè²»ç¨ç”³å‘Šæ›¸", "èª²ç¨æœŸé–“åˆ†", "åŸºæº–æœŸé–“"],
                "exclude_keywords": ["æ·»ä»˜è³‡æ–™", "ã‚¤ãƒ¡ãƒ¼ã‚¸æ·»ä»˜", "è³‡æ–™", "å—ä¿¡é€šçŸ¥", "ç´ä»˜åŒºåˆ†ç•ªå·é€šçŸ¥"],
                "filename_keywords": ["æ¶ˆè²»ç¨åŠã³åœ°æ–¹æ¶ˆè²»ç¨ç”³å‘Š", "æ¶ˆè²»ç¨ç”³å‘Š", "åœ°æ–¹æ¶ˆè²»ç¨ç”³å‘Š"]
            },
            
            "3002_æ·»ä»˜è³‡æ–™_æ¶ˆè²»ç¨": {
                "priority": 220,  # 0002ã‚ˆã‚Šé«˜ã„å„ªå…ˆåº¦ã«è¨­å®š
                "highest_priority_conditions": [
                    # æ–°ã—ã„æœ€å„ªå…ˆæ¡ä»¶: æ·»ä»˜æ›¸é¡ã€æ¶ˆè²»ç¨ã€æ·»ä»˜æ›¸é¡é€ä»˜æ›¸ã®3ã¤ã§ç¢ºå®Ÿã«åˆ¤å®š
                    AndCondition(["æ·»ä»˜æ›¸é¡", "æ¶ˆè²»ç¨", "æ·»ä»˜æ›¸é¡é€ä»˜æ›¸"], "all"),
                    # ä¿®æ­£æŒ‡ç¤ºæ›¸ã«åŸºã¥ããƒ•ã‚¡ã‚¤ãƒ«åæœ€å„ªå…ˆæ¡ä»¶ã‚’è¿½åŠ 
                    AndCondition(["ã‚¤ãƒ¡ãƒ¼ã‚¸æ·»ä»˜æ›¸é¡(æ³•äººæ¶ˆè²»ç¨ç”³å‘Š)"], "any"),  # å˜ç‹¬æœ€å„ªå…ˆ
                    AndCondition(["æ·»ä»˜è³‡æ–™", "æ¶ˆè²»ç¨ç”³å‘Š", "ã‚¤ãƒ¡ãƒ¼ã‚¸æ·»ä»˜"], "all"),
                    AndCondition(["æ·»ä»˜æ›¸é¡", "æ³•äººæ¶ˆè²»ç¨ç”³å‘Š"], "all"),
                    AndCondition(["ã‚¤ãƒ¡ãƒ¼ã‚¸æ·»ä»˜æ›¸é¡(æ³•äººæ¶ˆè²»ç¨ç”³å‘Š)", "æ·»ä»˜è³‡æ–™"], "all")
                ],
                "exact_keywords": [
                    "æ·»ä»˜æ›¸é¡é€ä»˜æ›¸", "æ·»ä»˜æ›¸é¡", "æ¶ˆè²»ç¨",
                    "æ¶ˆè²»ç¨ æ·»ä»˜è³‡æ–™", "æ·»ä»˜è³‡æ–™ æ¶ˆè²»ç¨", "ã‚¤ãƒ¡ãƒ¼ã‚¸æ·»ä»˜æ›¸é¡(æ³•äººæ¶ˆè²»ç¨ç”³å‘Š)",
                    "ã‚¤ãƒ¡ãƒ¼ã‚¸æ·»ä»˜æ›¸é¡ æ¶ˆè²»ç¨", "æ·»ä»˜æ›¸é¡ æ¶ˆè²»ç¨"
                ],
                "partial_keywords": ["æ·»ä»˜è³‡æ–™", "æ¶ˆè²»ç¨ è³‡æ–™", "ã‚¤ãƒ¡ãƒ¼ã‚¸æ·»ä»˜", "æ·»ä»˜æ›¸é¡"],
                "exclude_keywords": [
                    "æ¶ˆè²»ç¨åŠã³åœ°æ–¹æ¶ˆè²»ç¨ç”³å‘Š", "æ¶ˆè²»ç¨ç”³å‘Šæ›¸", "ç”³å‘Š(ä¸€èˆ¬ãƒ»æ³•äºº)", 
                    "æ³•äººç¨ç”³å‘Š", "å†…å›½æ³•äºº", "ç¢ºå®šç”³å‘Š", "å—ä¿¡é€šçŸ¥", "ç´ä»˜åŒºåˆ†ç•ªå·é€šçŸ¥"
                ],
                "filename_keywords": ["ã‚¤ãƒ¡ãƒ¼ã‚¸æ·»ä»˜æ›¸é¡", "æ·»ä»˜æ›¸é¡", "æ³•äººæ¶ˆè²»ç¨"]
            },
            
            "3003_å—ä¿¡é€šçŸ¥": {
                "priority": 130,
                "highest_priority_conditions": [
                    AndCondition(["ãƒ¡ãƒ¼ãƒ«è©³ç´°", "ç¨®ç›® æ¶ˆè²»ç¨ç”³å‘Šæ›¸"], "all"),
                    AndCondition(["å—ä»˜ç•ªå·", "æ¶ˆè²»ç¨åŠã³åœ°æ–¹æ¶ˆè²»ç¨", "å—ä»˜æ—¥æ™‚"], "all"),
                    AndCondition(["æå‡ºå…ˆ", "ç¨å‹™ç½²", "æ¶ˆè²»ç¨ç”³å‘Šæ›¸"], "all"),
                    AndCondition(["é€ä¿¡ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘ä»˜ã‘ã¾ã—ãŸ", "æ¶ˆè²»ç¨"], "all")
                ],
                "exact_keywords": ["æ¶ˆè²»ç¨ å—ä¿¡é€šçŸ¥", "å—ä¿¡é€šçŸ¥ æ¶ˆè²»ç¨"],
                "partial_keywords": ["å—ä¿¡é€šçŸ¥", "å›½ç¨é›»å­ç”³å‘Š", "ãƒ¡ãƒ¼ãƒ«è©³ç´°"],
                "exclude_keywords": ["æ³•äººç¨åŠã³åœ°æ–¹æ³•äººç¨ç”³å‘Šæ›¸", "ç´ä»˜åŒºåˆ†ç•ªå·é€šçŸ¥"],
                "filename_keywords": ["å—ä¿¡é€šçŸ¥", "æ¶ˆè²»ç¨"]
            },
            
            "3004_ç´ä»˜æƒ…å ±": {
                "priority": 130,
                "highest_priority_conditions": [
                    AndCondition(["ãƒ¡ãƒ¼ãƒ«è©³ç´°ï¼ˆç´ä»˜åŒºåˆ†ç•ªå·é€šçŸ¥ï¼‰", "æ¶ˆè²»ç¨åŠåœ°æ–¹æ¶ˆè²»ç¨"], "all"),
                    AndCondition(["ç´ä»˜åŒºåˆ†ç•ªå·é€šçŸ¥", "ç¨ç›® æ¶ˆè²»ç¨åŠåœ°æ–¹æ¶ˆè²»ç¨"], "all"),
                    AndCondition(["ç´ä»˜å…ˆ", "ç¨å‹™ç½²", "æ¶ˆè²»ç¨åŠåœ°æ–¹æ¶ˆè²»ç¨"], "all"),
                    AndCondition(["ç´ä»˜å†…å®¹ã‚’ç¢ºèªã—", "æ¶ˆè²»ç¨"], "all")
                ],
                "exact_keywords": ["æ¶ˆè²»ç¨ ç´ä»˜æƒ…å ±", "ç´ä»˜æƒ…å ± æ¶ˆè²»ç¨", "æ¶ˆè²»ç¨ ç´ä»˜åŒºåˆ†ç•ªå·é€šçŸ¥"],
                "partial_keywords": ["ç´ä»˜æƒ…å ±", "ç´ä»˜æ›¸", "ç´ä»˜åŒºåˆ†ç•ªå·é€šçŸ¥"],
                "exclude_keywords": ["æ³•äººç¨åŠåœ°æ–¹æ³•äººç¨", "å—ä¿¡é€šçŸ¥"],
                "filename_keywords": ["ç´ä»˜æƒ…å ±", "æ¶ˆè²»ç¨"]
            },
            
            # ===== 5000ç•ªå° - ä¼šè¨ˆæ›¸é¡ï¼ˆä¿®æ­£ç‰ˆï¼‰ =====
            "5001_æ±ºç®—æ›¸": {
                "priority": 130,  # æ®‹é«˜è©¦ç®—è¡¨ã‚ˆã‚Šä½ã„å„ªå…ˆåº¦
                "highest_priority_conditions": [
                    AndCondition(["æ±ºç®—å ±å‘Šæ›¸"], "any"),
                    AndCondition(["è²¸å€Ÿå¯¾ç…§è¡¨", "æç›Šè¨ˆç®—æ›¸"], "all")
                ],
                "exact_keywords": ["æ±ºç®—æ›¸", "æ±ºç®—å ±å‘Šæ›¸", "è²¸å€Ÿå¯¾ç…§è¡¨", "æç›Šè¨ˆç®—æ›¸"],
                "partial_keywords": ["æ±ºç®—", "B/S", "P/L"],
                "exclude_keywords": ["æ®‹é«˜è©¦ç®—è¡¨", "è©¦ç®—è¡¨", "æ®‹é«˜è©¦ç®—"],  # è©¦ç®—è¡¨é–¢é€£ã‚’é™¤å¤–
                "filename_keywords": ["æ±ºç®—æ›¸", "æ±ºç®—å ±å‘Šæ›¸"]
            },
            
            "5002_ç·å‹˜å®šå…ƒå¸³": {
                "priority": 140,  # æœ€é«˜å„ªå…ˆåº¦
                "highest_priority_conditions": [
                    AndCondition(["ç·å‹˜å®šå…ƒå¸³"], "any")
                ],
                "exact_keywords": ["ç·å‹˜å®šå…ƒå¸³"],
                "partial_keywords": ["ç·å‹˜å®š", "å…ƒå¸³"],
                "exclude_keywords": ["è£œåŠ©å…ƒå¸³", "è£œåŠ©", "å†…å›½æ³•äºº", "ç¢ºå®šç”³å‘Š", "01_å†…å›½æ³•äºº"],  # æ³•äººç¨ç”³å‘Šæ›¸ã‚’é™¤å¤–
                "filename_keywords": ["ç·å‹˜å®šå…ƒå¸³", "ç·å‹˜å®š"]
            },
            
            "5003_è£œåŠ©å…ƒå¸³": {
                "priority": 135,
                "highest_priority_conditions": [
                    AndCondition(["è£œåŠ©å…ƒå¸³"], "any")
                ],
                "exact_keywords": ["è£œåŠ©å…ƒå¸³"],
                "partial_keywords": ["è£œåŠ©", "å…ƒå¸³"],
                "exclude_keywords": ["ç·å‹˜å®š"],
                "filename_keywords": ["è£œåŠ©å…ƒå¸³", "è£œåŠ©"]
            },
            
            "5004_æ®‹é«˜è©¦ç®—è¡¨": {
                "priority": 140,  # æœ€é«˜å„ªå…ˆåº¦ï¼ˆæ±ºç®—æ›¸ã‚ˆã‚Šé«˜ã„ï¼‰
                "highest_priority_conditions": [
                    AndCondition(["æ®‹é«˜è©¦ç®—è¡¨"], "any"),
                    AndCondition(["è©¦ç®—è¡¨"], "any")
                ],
                "exact_keywords": ["æ®‹é«˜è©¦ç®—è¡¨", "è©¦ç®—è¡¨"],
                "partial_keywords": [
                    "æ®‹é«˜è©¦ç®—", "å€Ÿæ–¹é‡‘é¡", "è²¸æ–¹é‡‘é¡", "æœŸæœ«æ®‹é«˜", "æ§‹æˆæ¯”",
                    "åˆè¨ˆæ®‹é«˜", "æœˆæ¬¡è©¦ç®—", "å¹´æ¬¡è©¦ç®—", "å‹˜å®šæ®‹é«˜"
                ],
                "exclude_keywords": [],
                "filename_keywords": ["æ®‹é«˜è©¦ç®—è¡¨", "è©¦ç®—è¡¨"]
            },
            
            "5005_ä»•è¨³å¸³": {
                "priority": 135,
                "highest_priority_conditions": [
                    AndCondition(["ä»•è¨³å¸³"], "any")
                ],
                "exact_keywords": ["ä»•è¨³å¸³"],
                "partial_keywords": ["ä»•è¨³"],
                "exclude_keywords": [],
                "filename_keywords": ["ä»•è¨³å¸³", "ä»•è¨³"]
            },
            
            # ===== 6000ç•ªå° - å›ºå®šè³‡ç”£é–¢é€£ï¼ˆä¿®æ­£ç‰ˆï¼‰ =====
            "6001_å›ºå®šè³‡ç”£å°å¸³": {
                "priority": 135,
                "highest_priority_conditions": [
                    AndCondition(["å›ºå®šè³‡ç”£å°å¸³"], "any")
                ],
                "exact_keywords": ["å›ºå®šè³‡ç”£å°å¸³"],
                "partial_keywords": ["å›ºå®šè³‡ç”£", "è³‡ç”£å°å¸³"],
                "exclude_keywords": [],
                "filename_keywords": ["å›ºå®šè³‡ç”£å°å¸³"]
            },
            
            "6002_ä¸€æ‹¬å„Ÿå´è³‡ç”£æ˜ç´°è¡¨": {
                "priority": 100,  # æœ€é«˜å„ªå…ˆåº¦ï¼ˆ100ã«å¤‰æ›´ï¼‰
                "highest_priority_conditions": [
                    AndCondition(["ä¸€æ‹¬å„Ÿå´è³‡ç”£æ˜ç´°è¡¨"], "any"),
                    AndCondition(["ä¸€æ‹¬å„Ÿå´"], "any"),
                    AndCondition(["å„Ÿå´è³‡ç”£æ˜ç´°"], "any")
                ],
                "exact_keywords": ["ä¸€æ‹¬å„Ÿå´è³‡ç”£æ˜ç´°è¡¨"],
                "partial_keywords": ["ä¸€æ‹¬å„Ÿå´", "å„Ÿå´è³‡ç”£æ˜ç´°", "ä¸€æ‹¬å„Ÿå´è³‡ç”£", "å„Ÿå´æ˜ç´°"],
                "exclude_keywords": ["å°‘é¡"],
                "filename_keywords": ["ä¸€æ‹¬å„Ÿå´è³‡ç”£æ˜ç´°è¡¨", "ä¸€æ‹¬å„Ÿå´", "å„Ÿå´è³‡ç”£æ˜ç´°"],
                "meta": {"no_split": True, "asset_document": True, "lock_layer": "C"}
            },
            
            "6003_å°‘é¡æ¸›ä¾¡å„Ÿå´è³‡ç”£æ˜ç´°è¡¨": {
                "priority": 140,  # æœ€é«˜å„ªå…ˆåº¦
                "highest_priority_conditions": [
                    AndCondition(["å°‘é¡æ¸›ä¾¡å„Ÿå´è³‡ç”£æ˜ç´°è¡¨"], "any"),
                    AndCondition(["å°‘é¡æ¸›ä¾¡"], "any"),
                    AndCondition(["å°‘é¡"], "any")
                ],
                "exact_keywords": ["å°‘é¡æ¸›ä¾¡å„Ÿå´è³‡ç”£æ˜ç´°è¡¨"],
                "partial_keywords": [
                    "å°‘é¡æ¸›ä¾¡å„Ÿå´", "å°‘é¡å„Ÿå´", "å°‘é¡æ¸›ä¾¡å„Ÿå´è³‡ç”£", "å°‘é¡è³‡ç”£",
                    "å–å¾—ä¾¡é¡", "æé‡‘ç®—å…¥", "30ä¸‡å††æœªæº€", "æ¸›ä¾¡å„Ÿå´è³‡ç”£", "æ˜ç´°è¡¨",
                    "å°‘é¡å›ºå®šè³‡ç”£", "å„Ÿå´è³‡ç”£æ˜ç´°", "ä¸€æ™‚æé‡‘ç®—å…¥"
                ],
                "exclude_keywords": ["ä¸€æ‹¬"],
                "filename_keywords": [],  # ãƒ•ã‚¡ã‚¤ãƒ«ååˆ¤å®šã‚’å‰Šé™¤ã—ã¦OCRå†…å®¹ãƒ™ãƒ¼ã‚¹ã«çµ±ä¸€
                "meta": {"no_split": True, "asset_document": True, "lock_layer": "C"}
            },
            
            "6003_å›ºå®šè³‡ç”£å°å¸³å°‘é¡": {
                "priority": 120,  # ä¸­å„ªå…ˆåº¦ï¼ˆANDã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¡ä»¶ï¼‰
                "highest_priority_conditions": [
                    AndCondition(["å›ºå®šè³‡ç”£å°å¸³", "å–å¾—ä¾¡é¡"], "all"),
                    AndCondition(["è³‡ç”£ç•ªå·", "è€ç”¨å¹´æ•°"], "all"),
                    AndCondition(["ä¾›ç”¨é–‹å§‹", "æ¸›ä¾¡å„Ÿå´è³‡ç”£"], "all"),
                    AndCondition(["æ˜ç´°è¡¨", "å°‘é¡"], "all")
                ],
                "exact_keywords": [],
                "partial_keywords": [
                    "å›ºå®šè³‡ç”£å°å¸³", "è³‡ç”£ç•ªå·", "è€ç”¨å¹´æ•°", "ä¾›ç”¨é–‹å§‹",
                    "å–å¾—ä¾¡é¡", "æ¸›ä¾¡å„Ÿå´è³‡ç”£", "æ˜ç´°è¡¨", "å°‘é¡"
                ],
                "exclude_keywords": ["ä¸€æ‹¬"],
                "filename_keywords": [],
                "output_override": "6003_å°‘é¡æ¸›ä¾¡å„Ÿå´è³‡ç”£æ˜ç´°è¡¨",  # å‡ºåŠ›ã¯6003ã«çµ±ä¸€
                "meta": {"no_split": True, "asset_document": True, "lock_layer": "C"}
            },
            
            # ===== 7000ç•ªå° - ç¨åŒºåˆ†é–¢é€£ï¼ˆä¿®æ­£ç‰ˆï¼‰ =====
            "7001_å‹˜å®šç§‘ç›®åˆ¥ç¨åŒºåˆ†é›†è¨ˆè¡¨": {
                "priority": 140,  # æœ€é«˜å„ªå…ˆåº¦
                "highest_priority_conditions": [
                    AndCondition(["å‹˜å®šç§‘ç›®åˆ¥ç¨åŒºåˆ†é›†è¨ˆè¡¨"], "any")
                ],
                "exact_keywords": ["å‹˜å®šç§‘ç›®åˆ¥ç¨åŒºåˆ†é›†è¨ˆè¡¨"],
                "partial_keywords": ["å‹˜å®šç§‘ç›®åˆ¥ç¨åŒºåˆ†", "ç§‘ç›®åˆ¥ç¨åŒºåˆ†"],
                "exclude_keywords": ["ã‚¤ãƒ¡ãƒ¼ã‚¸æ·»ä»˜æ›¸é¡", "æ·»ä»˜è³‡æ–™", "æ³•äººæ¶ˆè²»ç¨ç”³å‘Š"],  # æ·»ä»˜è³‡æ–™ç³»ã‚’é™¤å¤–
                "filename_keywords": ["å‹˜å®šç§‘ç›®åˆ¥ç¨åŒºåˆ†é›†è¨ˆè¡¨"]
            },
            
            "7002_ç¨åŒºåˆ†é›†è¨ˆè¡¨": {
                "priority": 135,
                "highest_priority_conditions": [
                    AndCondition(["ç¨åŒºåˆ†é›†è¨ˆè¡¨"], "any")
                ],
                "exact_keywords": ["ç¨åŒºåˆ†é›†è¨ˆè¡¨"],
                "partial_keywords": ["ç¨åŒºåˆ†é›†è¨ˆ", "åŒºåˆ†é›†è¨ˆ"],
                "exclude_keywords": ["å‹˜å®šç§‘ç›®åˆ¥", "ç§‘ç›®åˆ¥"],
                "filename_keywords": ["ç¨åŒºåˆ†é›†è¨ˆè¡¨"]
            }
        }

    def _log(self, message: str, level: str = "INFO"):
        """ãƒ­ã‚°å‡ºåŠ›"""
        timestamp = datetime.datetime.now().strftime("%H:%M:%S.%f")[:-3]
        log_entry = f"[{timestamp}] [{level}] {message}"
        
        # å†…éƒ¨ãƒ­ã‚°ãƒªã‚¹ãƒˆã«è¿½åŠ 
        self.processing_log.append(log_entry)
        
        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ãŒã‚ã‚Œã°å‘¼ã³å‡ºã—
        if self.log_callback:
            self.log_callback(log_entry)
        
        # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
        if self.debug_mode:
            print(log_entry)

    def _log_debug(self, message: str):
        """ãƒ‡ãƒãƒƒã‚°ãƒ¬ãƒ™ãƒ«ã®ãƒ­ã‚°"""
        if self.debug_mode:
            self._log(message, "DEBUG")

    def _check_highest_priority_conditions(self, text: str, filename: str) -> Optional[ClassificationResult]:
        """æœ€å„ªå…ˆæ¡ä»¶ï¼ˆANDæ¡ä»¶ï¼‰ã‚’ãƒã‚§ãƒƒã‚¯"""
        combined_text = f"{text} {filename}"
        
        self._log("æœ€å„ªå…ˆANDæ¡ä»¶åˆ¤å®šé–‹å§‹")
        
        # å„ªå…ˆåº¦é †ã§ãƒã‚§ãƒƒã‚¯
        sorted_rules = sorted(self.classification_rules_v5.items(), 
                            key=lambda x: x[1].get("priority", 0), reverse=True)
        
        for doc_type, rules in sorted_rules:
            highest_priority_conditions = rules.get("highest_priority_conditions", [])
            
            if not highest_priority_conditions:
                continue
            
            for i, condition in enumerate(highest_priority_conditions):
                is_match, matched_keywords = condition.check_match(combined_text)
                
                if is_match:
                    self._log(f"æœ€å„ªå…ˆANDæ¡ä»¶ä¸€è‡´: {doc_type} (æ¡ä»¶{i+1})")
                    self._log_debug(f"ãƒãƒƒãƒã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {matched_keywords}")
                    self._log_debug(f"ãƒãƒƒãƒã‚¿ã‚¤ãƒ—: {condition.match_type}")
                    
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
                    meta_data = rules.get("meta", {})
                    
                    result = ClassificationResult(
                        document_type=doc_type,
                        confidence=1.0,  # æœ€å„ªå…ˆãªã®ã§ä¿¡é ¼åº¦100%
                        matched_keywords=matched_keywords,
                        classification_method="highest_priority_and_condition",
                        debug_steps=[],
                        processing_log=self.processing_log.copy(),
                        meta=meta_data
                    )
                    
                    # 6002/6003ã®å ´åˆã€å±¤Cå››é‡ãƒ­ãƒƒã‚¯è­¦å‘Š
                    if doc_type.startswith(('6002_', '6003_')):
                        self._log(f"[6002/6003 Lock C] Asset document detected: {doc_type}")
                    
                    return result
        
        self._log_debug("æœ€å„ªå…ˆANDæ¡ä»¶ãƒãƒƒãƒãªã— - é€šå¸¸åˆ†é¡å‡¦ç†ã«ç§»è¡Œ")
        return None

    def classify_document_v5(self, text: str, filename: str = "", job_context=None) -> ClassificationResult:
        """v5.0 æ›¸é¡åˆ†é¡ï¼ˆANDæ¡ä»¶å¯¾å¿œç‰ˆï¼‰+ å—ä¿¡é€šçŸ¥é€£ç•ªå‡¦ç†å¼·åˆ¶å®Ÿè¡Œ"""
        self.processing_log = []  # ãƒ­ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
        self.current_filename = filename
        
        # ğŸ”¥ Bundleåˆ†å‰²ãƒ•ã‚¡ã‚¤ãƒ«ã®ç‰¹åˆ¥ãƒ­ã‚°ï¼ˆæ®µéš1ï¼‰
        if filename.startswith("__split_"):
            print(f"[BUNDLE_SPLIT_DEBUG] Bundleåˆ†å‰²ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {filename}")
            print(f"[BUNDLE_SPLIT_DEBUG] JobContextå­˜åœ¨: {job_context is not None}")
            if job_context:
                print(f"[BUNDLE_SPLIT_DEBUG] JobContextå†…å®¹: {type(job_context)}")
                municipality_sets = getattr(job_context, 'current_municipality_sets', None)
                print(f"[BUNDLE_SPLIT_DEBUG] municipality_sets: {municipality_sets}")
        
        self._log(f"æ›¸é¡åˆ†é¡é–‹å§‹ (v5.0): {filename}")
        
        # ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†
        text_cleaned = self._preprocess_text(text)
        filename_cleaned = self._preprocess_text(filename)
        
        self._log_debug(f"å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆé•·: {len(text)} â†’ å‰å‡¦ç†å¾Œ: {len(text_cleaned)}")
        self._log_debug(f"ãƒ•ã‚¡ã‚¤ãƒ«å: {filename} â†’ å‰å‡¦ç†å¾Œ: {filename_cleaned}")
        
        # æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆã®ä¸€éƒ¨ã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        if text_cleaned:
            preview = text_cleaned[:200] + "..." if len(text_cleaned) > 200 else text_cleaned
            self._log_debug(f"ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹: {preview}")
        
        # ãƒã‚°ä¿®æ­£ä¾é ¼æ›¸: D-2 åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥å°‚ç”¨åˆ¤å®šï¼ˆæ–°è¦è¿½åŠ ï¼‰
        municipality_info = self._extract_municipality_info_from_text(text_cleaned, filename_cleaned)
        prefecture_code, municipality_code = municipality_info
        
        local_tax_result = self._classify_local_tax_receipt(text_cleaned, filename_cleaned, prefecture_code, municipality_code)
        if local_tax_result:
            # ğŸ”¥ åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥ã®ç‰¹åˆ¥ãƒ­ã‚°ï¼ˆæ®µéš1ï¼‰
            if (filename.startswith("__split_") and 
                hasattr(local_tax_result, 'classification_method') and
                local_tax_result.classification_method == "local_tax_receipt_detection_ocr"):
                print(f"[LOCAL_TAX_RECEIPT_DEBUG] åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥æ¤œå‡ºï¼")
                print(f"[LOCAL_TAX_RECEIPT_DEBUG] åˆ†é¡çµæœ: {local_tax_result.document_type}")
                print(f"[LOCAL_TAX_RECEIPT_DEBUG] åˆ¤å®šæ–¹æ³•: {local_tax_result.classification_method}")
                print(f"[LOCAL_TAX_RECEIPT_DEBUG] JobContext for numbering: {job_context is not None}")
            
            # ğŸ”¥ ä¿®æ­£æŒ‡ç¤ºæ›¸å¯¾å¿œ: å—ä¿¡é€šçŸ¥ã®å ´åˆã€å³åº§ã«é€£ç•ªå‡¦ç†ã‚’å®Ÿè¡Œ
            if job_context is not None:
                print(f"[RECEIPT_NUMBERING_DEBUG] é€£ç•ªå‡¦ç†é–‹å§‹")
            final_result = self._apply_receipt_numbering_if_needed(local_tax_result, text_cleaned, job_context)
            return final_result
        
        # ä¿®æ­£æŒ‡ç¤ºæ›¸: ä¿®æ­£3 - ç´ä»˜æƒ…å ±ãƒ»å—ä¿¡é€šçŸ¥ã®åˆ¤åˆ¥å¼·åŒ–
        enhanced_result = self._check_enhanced_payment_receipt_detection(text_cleaned, filename_cleaned)
        if enhanced_result:
            # ğŸ”¥ åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥ã®ç‰¹åˆ¥ãƒ­ã‚°ï¼ˆæ®µéš1ï¼‰
            if (filename.startswith("__split_") and 
                hasattr(enhanced_result, 'classification_method') and
                enhanced_result.classification_method == "local_tax_receipt_detection_ocr"):
                print(f"[LOCAL_TAX_RECEIPT_DEBUG] åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥æ¤œå‡ºï¼")
                print(f"[LOCAL_TAX_RECEIPT_DEBUG] åˆ†é¡çµæœ: {enhanced_result.document_type}")
                print(f"[LOCAL_TAX_RECEIPT_DEBUG] åˆ¤å®šæ–¹æ³•: {enhanced_result.classification_method}")
                print(f"[LOCAL_TAX_RECEIPT_DEBUG] JobContext for numbering: {job_context is not None}")
            
            # ğŸ”¥ ä¿®æ­£æŒ‡ç¤ºæ›¸å¯¾å¿œ: å—ä¿¡é€šçŸ¥ã®å ´åˆã€å³åº§ã«é€£ç•ªå‡¦ç†ã‚’å®Ÿè¡Œ
            if job_context is not None:
                print(f"[RECEIPT_NUMBERING_DEBUG] é€£ç•ªå‡¦ç†é–‹å§‹")
            final_result = self._apply_receipt_numbering_if_needed(enhanced_result, text_cleaned, job_context)
            return final_result
        
        # æœ€å„ªå…ˆANDæ¡ä»¶åˆ¤å®š
        priority_result = self._check_highest_priority_conditions(text_cleaned, filename_cleaned)
        if priority_result:
            # ğŸ”¥ åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥ã®ç‰¹åˆ¥ãƒ­ã‚°ï¼ˆæ®µéš1ï¼‰
            if (filename.startswith("__split_") and 
                hasattr(priority_result, 'classification_method') and
                priority_result.classification_method == "local_tax_receipt_detection_ocr"):
                print(f"[LOCAL_TAX_RECEIPT_DEBUG] åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥æ¤œå‡ºï¼")
                print(f"[LOCAL_TAX_RECEIPT_DEBUG] åˆ†é¡çµæœ: {priority_result.document_type}")
                print(f"[LOCAL_TAX_RECEIPT_DEBUG] åˆ¤å®šæ–¹æ³•: {priority_result.classification_method}")
                print(f"[LOCAL_TAX_RECEIPT_DEBUG] JobContext for numbering: {job_context is not None}")
            
            # ğŸ”¥ ä¿®æ­£æŒ‡ç¤ºæ›¸å¯¾å¿œ: å—ä¿¡é€šçŸ¥ã®å ´åˆã€å³åº§ã«é€£ç•ªå‡¦ç†ã‚’å®Ÿè¡Œ
            if job_context is not None:
                print(f"[RECEIPT_NUMBERING_DEBUG] é€£ç•ªå‡¦ç†é–‹å§‹")
            final_result = self._apply_receipt_numbering_if_needed(priority_result, text_cleaned, job_context)
            return final_result
        
        # é€šå¸¸ã®åˆ†é¡å‡¦ç†ï¼ˆå¾“æ¥ãƒ«ãƒ¼ãƒ«ã‚‚ç¶­æŒï¼‰
        standard_result = self._standard_classification(text_cleaned, filename_cleaned)
        
        # ğŸ”¥ åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥ã®ç‰¹åˆ¥ãƒ­ã‚°ï¼ˆæ®µéš1ï¼‰
        if (filename.startswith("__split_") and 
            hasattr(standard_result, 'classification_method') and
            standard_result.classification_method == "local_tax_receipt_detection_ocr"):
            print(f"[LOCAL_TAX_RECEIPT_DEBUG] åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥æ¤œå‡ºï¼")
            print(f"[LOCAL_TAX_RECEIPT_DEBUG] åˆ†é¡çµæœ: {standard_result.document_type}")
            print(f"[LOCAL_TAX_RECEIPT_DEBUG] åˆ¤å®šæ–¹æ³•: {standard_result.classification_method}")
            print(f"[LOCAL_TAX_RECEIPT_DEBUG] JobContext for numbering: {job_context is not None}")
        
        # ğŸ”¥ ä¿®æ­£æŒ‡ç¤ºæ›¸å¯¾å¿œ: å—ä¿¡é€šçŸ¥ã®å ´åˆã€å³åº§ã«é€£ç•ªå‡¦ç†ã‚’å®Ÿè¡Œ
        if job_context is not None:
            print(f"[RECEIPT_NUMBERING_DEBUG] é€£ç•ªå‡¦ç†é–‹å§‹")
        final_result = self._apply_receipt_numbering_if_needed(standard_result, text_cleaned, job_context)
        return final_result

    def _apply_receipt_numbering_if_needed(self, classification_result: ClassificationResult, 
                                         ocr_text: str, job_context) -> ClassificationResult:
        """
        ğŸ”¥ ä¿®æ­£æŒ‡ç¤ºæ›¸å¯¾å¿œ: å—ä¿¡é€šçŸ¥åˆ†é¡æ™‚ã®é€£ç•ªå‡¦ç†å¼·åˆ¶å®Ÿè¡Œ
        
        Args:
            classification_result: åŸºæœ¬åˆ†é¡çµæœ
            ocr_text: OCRãƒ†ã‚­ã‚¹ãƒˆ
            job_context: JobContextï¼ˆè‡ªæ²»ä½“ã‚»ãƒƒãƒˆæƒ…å ±ï¼‰
            
        Returns:
            ClassificationResult: é€£ç•ªå‡¦ç†å¾Œã®æœ€çµ‚åˆ†é¡çµæœ
        """
        from helpers.seq_policy import is_receipt_notice, is_pref_receipt, is_city_receipt
        
        base_code = classification_result.document_type
        
        # ğŸ”¥ æ®µéš1ï¼šè©³ç´°ãƒ­ã‚°è¿½åŠ  - é€£ç•ªå‡¦ç†å‘¼ã³å‡ºã—éƒ¨åˆ†
        if self.current_filename.startswith("__split_"):
            print(f"[RECEIPT_NUMBERING_DEBUG] é€£ç•ªå‡¦ç†é–‹å§‹")
            print(f"[RECEIPT_NUMBERING_DEBUG] å‡¦ç†å‰åˆ†é¡çµæœ: {classification_result.document_type}")
            
            # ğŸ”¥ åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥ã®å ´åˆã®ç‰¹åˆ¥å‡¦ç†ç¢ºèª
            if classification_result.document_type in ["1003_å—ä¿¡é€šçŸ¥", "2013_å—ä¿¡é€šçŸ¥"]:
                print(f"[RECEIPT_NUMBERING_DEBUG] åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥é€£ç•ªå‡¦ç†å¯¾è±¡ç¢ºèª")
                municipality_sets = getattr(job_context, 'current_municipality_sets', None)
                print(f"[RECEIPT_NUMBERING_DEBUG] è‡ªæ²»ä½“ã‚»ãƒƒãƒˆ: {municipality_sets}")
        
        print(f"[APPLY_NUMBERING_DEBUG] é€£ç•ªå‡¦ç†ãƒ¡ã‚½ãƒƒãƒ‰é–‹å§‹: {base_code}")
        
        # å—ä¿¡é€šçŸ¥åˆ¤å®š
        if not is_receipt_notice(base_code):
            print(f"[APPLY_NUMBERING_DEBUG] å—ä¿¡é€šçŸ¥ã§ã¯ãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {base_code}")
            return classification_result
            
        print(f"[APPLY_NUMBERING_DEBUG] å—ä¿¡é€šçŸ¥ç¢ºèªOK: {base_code}")
        
        # JobContextç¢ºèª
        if not job_context:
            print(f"[APPLY_NUMBERING_DEBUG] JobContextæœªè¨­å®šã®ãŸã‚é€£ç•ªå‡¦ç†ã‚¹ã‚­ãƒƒãƒ—")
            self._log(f"[BUNDLE_SPLIT_LOCAL_TAX] JobContextæœªè¨­å®šã®ãŸã‚é€£ç•ªå‡¦ç†ã‚¹ã‚­ãƒƒãƒ—: {base_code}")
            return classification_result
            
        print(f"[APPLY_NUMBERING_DEBUG] JobContextç¢ºèªOK")
        
        # municipality_setsç¢ºèª
        municipality_sets = getattr(job_context, 'current_municipality_sets', None)
        if not municipality_sets:
            print(f"[APPLY_NUMBERING_DEBUG] municipality_setsæœªè¨­å®šã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            return classification_result
            
        print(f"[APPLY_NUMBERING_DEBUG] municipality_setsç¢ºèªOK: {municipality_sets}")
        
        # åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥ã®ç‰¹åˆ¥å‡¦ç†ï¼ˆæ®µéš2å¼·åŒ–ç‰ˆï¼‰
        if (base_code in ["1003_å—ä¿¡é€šçŸ¥", "2013_å—ä¿¡é€šçŸ¥"] and 
            hasattr(classification_result, 'classification_method') and
            classification_result.classification_method == "local_tax_receipt_detection_ocr"):
            
            print(f"[RECEIPT_NUMBERING] åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥é€£ç•ªå‡¦ç†é–‹å§‹: {base_code}")
            
            if not job_context or not hasattr(job_context, 'current_municipality_sets'):
                print(f"[RECEIPT_NUMBERING] JobContextä¸è¶³ã‚¨ãƒ©ãƒ¼")
                return classification_result
                
            # é€£ç•ªå‡¦ç†ã‚’å¼·åˆ¶å®Ÿè¡Œ
            numbered_result = self._calculate_simplified_receipt_number(
                base_code, ocr_text, job_context
            )
            
            if numbered_result:
                print(f"[RECEIPT_NUMBERING] é€£ç•ªå‡¦ç†æˆåŠŸ: {base_code} â†’ {numbered_result}")
                # æ–°ã—ã„åˆ†é¡çµæœã‚’ä½œæˆ
                new_result = ClassificationResult(
                    document_type=numbered_result,
                    confidence=classification_result.confidence,
                    matched_keywords=classification_result.matched_keywords,
                    classification_method="local_tax_receipt_forced_numbering",
                    debug_steps=classification_result.debug_steps,
                    processing_log=classification_result.processing_log + [f"åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥é€£ç•ªå‡¦ç†: {base_code} -> {numbered_result}"],
                    original_doc_type_code=base_code
                )
                return new_result
            else:
                print(f"[RECEIPT_NUMBERING] é€£ç•ªå‡¦ç†å¤±æ•—: {base_code}")
        
        # ğŸ”¥ åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥ã®å ´åˆã®è©³ç´°å‡¦ç†
        if base_code in ["1003_å—ä¿¡é€šçŸ¥", "2013_å—ä¿¡é€šçŸ¥"]:
            print(f"[APPLY_NUMBERING_DEBUG] åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥é€£ç•ªå‡¦ç†é–‹å§‹: {base_code}")
            
        # åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥Bundleåˆ†å‰²æ™‚ã®è©³ç´°ãƒ­ã‚°
        if base_code.endswith("_å—ä¿¡é€šçŸ¥") and (base_code.startswith("1003") or base_code.startswith("2013")):
            self._log(f"[BUNDLE_SPLIT_LOCAL_TAX] åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥æ¤œå‡º: {base_code}")
            self._log(f"[BUNDLE_SPLIT_LOCAL_TAX] JobContext: {job_context}")
            if hasattr(job_context, 'current_municipality_sets'):
                self._log(f"[BUNDLE_SPLIT_LOCAL_TAX] municipality_sets: {job_context.current_municipality_sets}")
            else:
                self._log(f"[BUNDLE_SPLIT_LOCAL_TAX] municipality_setså±æ€§ãªã—")
            
        if not hasattr(job_context, 'current_municipality_sets') or not job_context.current_municipality_sets:
            self._log(f"[BUNDLE_SPLIT_LOCAL_TAX] current_municipality_setsä¸è¶³ã®ãŸã‚é€£ç•ªå‡¦ç†ã‚¹ã‚­ãƒƒãƒ—: {base_code}")
            return classification_result
            
        self._log(f"[RECEIPT_FORCED] å—ä¿¡é€šçŸ¥é€£ç•ªå‡¦ç†ã‚’å¼·åˆ¶å®Ÿè¡Œ: {base_code}")
        
        try:
            # ğŸ”¥ åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥ã®å ´åˆã®é€£ç•ªè¨ˆç®—å®Ÿè¡Œ
            if base_code in ["1003_å—ä¿¡é€šçŸ¥", "2013_å—ä¿¡é€šçŸ¥"]:
                # é€£ç•ªè¨ˆç®—å®Ÿè¡Œ
                final_code = self._calculate_simplified_receipt_number(base_code, ocr_text, job_context)
                
                if final_code and final_code != base_code:
                    print(f"[APPLY_NUMBERING_DEBUG] é€£ç•ªå‡¦ç†æˆåŠŸ: {base_code} â†’ {final_code}")
                else:
                    print(f"[APPLY_NUMBERING_DEBUG] é€£ç•ªå‡¦ç†å¤±æ•—ã¾ãŸã¯å¤‰æ›´ãªã—: {base_code}")
            else:
                # é€šå¸¸ã®é€£ç•ªè¨ˆç®—
                final_code = self._calculate_simplified_receipt_number(base_code, ocr_text, job_context)
            
            if final_code and final_code != base_code:
                # é€£ç•ªå‡¦ç†æˆåŠŸï¼šæ–°ã—ã„åˆ†é¡çµæœã‚’ä½œæˆ
                new_result = ClassificationResult(
                    document_type=final_code,
                    confidence=classification_result.confidence,
                    matched_keywords=classification_result.matched_keywords,
                    classification_method="receipt_forced_numbering",
                    debug_steps=classification_result.debug_steps,
                    processing_log=classification_result.processing_log + [f"é€£ç•ªå‡¦ç†é©ç”¨: {base_code} -> {final_code}"],
                    original_doc_type_code=base_code  # å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’ä¿å­˜
                )
                
                self._log(f"[RECEIPT_FORCED] é€£ç•ªå‡¦ç†æˆåŠŸ: {base_code} -> {final_code}")
                
                # ğŸ”¥ æ®µéš1ï¼šè©³ç´°ãƒ­ã‚°è¿½åŠ  - é€£ç•ªå‡¦ç†å®Œäº†
                if self.current_filename.startswith("__split_"):
                    print(f"[RECEIPT_NUMBERING_DEBUG] é€£ç•ªå‡¦ç†å®Œäº†")
                    print(f"[RECEIPT_NUMBERING_DEBUG] å‡¦ç†å¾Œåˆ†é¡çµæœ: {final_code}")
                
                return new_result
            else:
                self._log(f"[RECEIPT_FORCED] é€£ç•ªå‡¦ç†å¤±æ•—ã¾ãŸã¯ã‚¹ã‚­ãƒƒãƒ—: {base_code}")
                
                # ğŸ”¥ æ®µéš1ï¼šè©³ç´°ãƒ­ã‚°è¿½åŠ  - é€£ç•ªå‡¦ç†å®Œäº†ï¼ˆå¤‰æ›´ãªã—ï¼‰
                if self.current_filename.startswith("__split_"):
                    print(f"[RECEIPT_NUMBERING_DEBUG] é€£ç•ªå‡¦ç†å®Œäº†")
                    print(f"[RECEIPT_NUMBERING_DEBUG] å‡¦ç†å¾Œåˆ†é¡çµæœ: {base_code} (å¤‰æ›´ãªã—)")
                
                return classification_result
                
        except Exception as e:
            self._log(f"[RECEIPT_FORCED] é€£ç•ªå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            self._log_debug(f"[RECEIPT_FORCED] Traceback: {traceback.format_exc()}")
            return classification_result

    def _calculate_simplified_receipt_number(self, base_code: str, ocr_text: str, job_context) -> Optional[str]:
        """
        ç°¡ç´ åŒ–ã•ã‚ŒãŸå—ä¿¡é€šçŸ¥é€£ç•ªè¨ˆç®—ï¼ˆä¿®æ­£æŒ‡ç¤ºæ›¸å¯¾å¿œï¼‰
        
        Args:
            base_code: åŸºæœ¬åˆ†é¡ã‚³ãƒ¼ãƒ‰ï¼ˆä¾‹: "1003_å—ä¿¡é€šçŸ¥"ï¼‰
            ocr_text: OCRãƒ†ã‚­ã‚¹ãƒˆ
            job_context: JobContext
            
        Returns:
            Optional[str]: é€£ç•ªã‚³ãƒ¼ãƒ‰ï¼ˆä¾‹: "1013_å—ä¿¡é€šçŸ¥"ï¼‰ã€è¨ˆç®—å¤±æ•—æ™‚ã¯None
        """
        from helpers.seq_policy import is_pref_receipt, is_city_receipt
        
        # OCRã‹ã‚‰éƒ½é“åºœçœŒãƒ»å¸‚ç”ºæ‘ã‚’æŠ½å‡º
        if is_pref_receipt(base_code):
            # éƒ½é“åºœçœŒå—ä¿¡é€šçŸ¥
            prefecture = self._extract_prefecture_from_ocr_simple(ocr_text)
            if not prefecture:
                self._log_debug(f"[SIMPLIFIED_CALC] éƒ½é“åºœçœŒæŠ½å‡ºå¤±æ•—: {ocr_text[:100]}")
                return None
                
            set_number = job_context.get_set_index_for_pref(prefecture)
            if set_number is None:
                self._log_debug(f"[SIMPLIFIED_CALC] éƒ½é“åºœçœŒã‚»ãƒƒãƒˆç•ªå·å–å¾—å¤±æ•—: {prefecture}")
                return None
                
            # é€£ç•ªè¨ˆç®—: 1003 + (set_number - 1) * 10
            final_number = 1003 + (set_number - 1) * 10
            result_code = f"{final_number:04d}_å—ä¿¡é€šçŸ¥"
            
            self._log(f"[BUNDLE_SPLIT_LOCAL_TAX] éƒ½é“åºœçœŒé€£ç•ªå‡¦ç†æˆåŠŸ: OCR={prefecture} â†’ ã‚»ãƒƒãƒˆ{set_number} â†’ {result_code}")
            return result_code
            
        elif is_city_receipt(base_code):
            # å¸‚ç”ºæ‘å—ä¿¡é€šçŸ¥
            prefecture, city = self._extract_prefecture_city_from_ocr_simple(ocr_text)
            if not prefecture or not city:
                self._log_debug(f"[SIMPLIFIED_CALC] å¸‚ç”ºæ‘æŠ½å‡ºå¤±æ•—: pref={prefecture}, city={city}")
                return None
                
            set_number = job_context.get_set_index_for_city(prefecture, city)
            if set_number is None:
                self._log_debug(f"[SIMPLIFIED_CALC] å¸‚ç”ºæ‘ã‚»ãƒƒãƒˆç•ªå·å–å¾—å¤±æ•—: {prefecture} {city}")
                return None
                
            # æ±äº¬éƒ½ã‚¹ã‚­ãƒƒãƒ—å‡¦ç†
            tokyo_set = job_context.get_set_index_for_pref("æ±äº¬éƒ½")
            adjusted_set = set_number - 1 if (tokyo_set == 1 and set_number > 1) else set_number
            
            # é€£ç•ªè¨ˆç®—: 2003 + (adjusted_set - 1) * 10
            final_number = 2003 + (adjusted_set - 1) * 10
            result_code = f"{final_number:04d}_å—ä¿¡é€šçŸ¥"
            
            self._log(f"[BUNDLE_SPLIT_LOCAL_TAX] å¸‚ç”ºæ‘é€£ç•ªå‡¦ç†æˆåŠŸ: OCR={prefecture} {city} â†’ ã‚»ãƒƒãƒˆ{set_number}â†’èª¿æ•´{adjusted_set} â†’ {result_code}")
            return result_code
            
        return None

    def _extract_prefecture_from_ocr_simple(self, ocr_text: str) -> Optional[str]:
        """ç°¡ç´ åŒ–ã•ã‚ŒãŸéƒ½é“åºœçœŒæŠ½å‡ºï¼ˆæ®µéš3ç²¾åº¦å‘ä¸Šç‰ˆï¼‰"""
        print(f"[RECEIPT_CALC] OCRè§£æé–‹å§‹")
        print(f"[RECEIPT_CALC] æ¤œç´¢å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ: {ocr_text[:200]}...")
        
        # ç™ºè¡Œå…ƒãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©³ç´°åŒ–ï¼ˆæ®µéš3å¼·åŒ–ï¼‰
        prefecture_patterns = {
            "æ±äº¬éƒ½": ["æ±äº¬éƒ½", "æ¸¯éƒ½ç¨äº‹å‹™æ‰€", "éƒ½ç¨äº‹å‹™æ‰€"],
            "æ„›çŸ¥çœŒ": ["æ„›çŸ¥çœŒ", "æ±ä¸‰æ²³çœŒç¨äº‹å‹™æ‰€", "çœŒç¨äº‹å‹™æ‰€"],
            "ç¦å²¡çœŒ": ["ç¦å²¡çœŒ", "è¥¿ç¦å²¡çœŒç¨äº‹å‹™æ‰€", "ç¦å²¡çœŒç¨"]
        }
        
        # ã‚ˆã‚Šç²¾å¯†ãªçªåˆå‡¦ç†
        for prefecture, patterns in prefecture_patterns.items():
            for pattern in patterns:
                if pattern in ocr_text:
                    print(f"[RECEIPT_CALC] éƒ½é“åºœçœŒãƒãƒƒãƒ: {pattern} â†’ {prefecture}")
                    return prefecture
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªéƒ½é“åºœçœŒåæ¤œç´¢
        prefectures = ["æ±äº¬éƒ½", "æ„›çŸ¥çœŒ", "ç¦å²¡çœŒ", "å¤§é˜ªåºœ", "ç¥å¥ˆå·çœŒ"]
        for pref in prefectures:
            if pref in ocr_text:
                print(f"[RECEIPT_CALC] éƒ½é“åºœçœŒãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒãƒƒãƒ: {pref}")
                return pref
        
        print(f"[RECEIPT_CALC] éƒ½é“åºœçœŒãƒãƒƒãƒãªã—")
        return None

    def _extract_prefecture_city_from_ocr_simple(self, ocr_text: str) -> Tuple[Optional[str], Optional[str]]:
        """ç°¡ç´ åŒ–ã•ã‚ŒãŸéƒ½é“åºœçœŒãƒ»å¸‚ç”ºæ‘æŠ½å‡ºï¼ˆæ®µéš3ç²¾åº¦å‘ä¸Šç‰ˆï¼‰"""
        print(f"[RECEIPT_CALC] å¸‚ç”ºæ‘OCRè§£æé–‹å§‹")
        print(f"[RECEIPT_CALC] æ¤œç´¢å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ: {ocr_text[:200]}...")
        
        # æ®µéš3ï¼šå¸‚ç”ºæ‘ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©³ç´°åŒ–
        city_patterns = {
            "è’²éƒ¡å¸‚": ["è’²éƒ¡å¸‚", "è’²éƒ¡å¸‚å½¹æ‰€"],
            "ç¦å²¡å¸‚": ["ç¦å²¡å¸‚", "ç¦å²¡å¸‚é•·"]
        }
        
        # ã¾ãšéƒ½é“åºœçœŒã‚’æŠ½å‡º
        prefectures_cities = {
            "æ±äº¬éƒ½": ["æ¸¯åŒº", "æ–°å®¿åŒº", "æ¸‹è°·åŒº"],
            "æ„›çŸ¥çœŒ": ["è’²éƒ¡å¸‚", "åå¤å±‹å¸‚", "è±Šç”°å¸‚"],
            "ç¦å²¡çœŒ": ["ç¦å²¡å¸‚", "åŒ—ä¹å·å¸‚", "ä¹…ç•™ç±³å¸‚"],
            "å¤§é˜ªåºœ": ["å¤§é˜ªå¸‚", "å ºå¸‚", "æ±å¤§é˜ªå¸‚"],
            "ç¥å¥ˆå·çœŒ": ["æ¨ªæµœå¸‚", "å·å´å¸‚", "ç›¸æ¨¡åŸå¸‚"]
        }
        
        # ã‚ˆã‚Šç²¾å¯†ãªçªåˆå‡¦ç†ï¼ˆæ®µéš3å¼·åŒ–ï¼‰
        for pref, cities in prefectures_cities.items():
            if pref in ocr_text:
                # è©²å½“éƒ½é“åºœçœŒã®å¸‚ç”ºæ‘ã‚’æ¢ã™
                for city in cities:
                    # æ‹¡å¼µãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã®æ¤œç´¢
                    if city in city_patterns:
                        for pattern in city_patterns[city]:
                            if pattern in ocr_text:
                                print(f"[RECEIPT_CALC] å¸‚ç”ºæ‘ãƒãƒƒãƒ: {pattern} â†’ {pref}({city})")
                                return pref, city
                    elif city in ocr_text:
                        print(f"[RECEIPT_CALC] å¸‚ç”ºæ‘åŸºæœ¬ãƒãƒƒãƒ: {city} â†’ {pref}({city})")
                        return pref, city
                
                # éƒ½é“åºœçœŒã¯è¦‹ã¤ã‹ã£ãŸãŒå¸‚ç”ºæ‘ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
                print(f"[RECEIPT_CALC] éƒ½é“åºœçœŒãƒãƒƒãƒã€å¸‚ç”ºæ‘æ¤œç´¢ä¸­: {pref}")
                # å½¹æ‰€ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å¸‚åã‚’æŠ½å‡ºã‚’è©¦è¡Œ
                import re
                city_match = re.search(r'([^çœŒéƒ½åºœé“å¸‚åŒºç”ºæ‘]+(?:å¸‚|åŒº|ç”º|æ‘))å½¹æ‰€', ocr_text)
                if city_match:
                    found_city = city_match.group(1)
                    return pref, found_city
                    
        return None, None

    def _standard_classification(self, text: str, filename: str) -> ClassificationResult:
        """æ¨™æº–åˆ†é¡å‡¦ç†ï¼ˆå¾“æ¥ãƒ«ãƒ¼ãƒ«ï¼‰"""
        best_match = None
        best_score = 0
        best_keywords = []
        best_method = "standard_keyword_matching"
        debug_steps = []
        
        self._log("æ¨™æº–åˆ†é¡ãƒ«ãƒ¼ãƒ«è©•ä¾¡é–‹å§‹")
        
        # å„åˆ†é¡ãƒ«ãƒ¼ãƒ«ã«å¯¾ã—ã¦ã‚¹ã‚³ã‚¢è¨ˆç®—
        for doc_type, rules in self.classification_rules_v5.items():
            self._log_debug(f"è©•ä¾¡ä¸­: {doc_type} (å„ªå…ˆåº¦: {rules.get('priority', 5)})")
            
            # ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ•ã‚¡ã‚¤ãƒ«åã‚’åˆ†ã‘ã¦ã‚¹ã‚³ã‚¢è¨ˆç®—
            text_score, text_keywords = self._calculate_score(text, rules, "ãƒ†ã‚­ã‚¹ãƒˆ")
            filename_score, filename_keywords = self._calculate_filename_score(filename, rules)
            
            # ç·åˆã‚¹ã‚³ã‚¢ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã‚’é‡è¦–ï¼‰
            total_score = text_score + (filename_score * 1.5)
            combined_keywords = text_keywords + filename_keywords
            
            # é™¤å¤–åˆ¤å®šãƒã‚§ãƒƒã‚¯
            excluded = False
            exclude_reason = ""
            
            # æœ€å„ªå…ˆæ¡ä»¶ãŒä¸€è‡´ã—ã¦ã„ã‚‹å ´åˆã¯é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ç„¡è¦–
            has_highest_priority = any(
                condition.check_match(f"{text} {filename}")[0] 
                for condition in rules.get("highest_priority_conditions", [])
            )
            
            if not has_highest_priority:
                # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
                for exclude_keyword in rules.get("exclude_keywords", []):
                    if exclude_keyword in text or exclude_keyword in filename:
                        excluded = True
                        exclude_reason = f"é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{exclude_keyword}' ã‚’æ¤œå‡º"
                        break
            
            # ãƒ‡ãƒãƒƒã‚°ã‚¹ãƒ†ãƒƒãƒ—è¨˜éŒ²
            step = ClassificationStep(
                document_type=doc_type,
                score=total_score,
                matched_keywords=combined_keywords,
                excluded=excluded,
                exclude_reason=exclude_reason,
                method="standard"
            )
            debug_steps.append(step)
            
            # ãƒ­ã‚°å‡ºåŠ›
            if excluded:
                self._log_debug(f"  â†’ {doc_type}: é™¤å¤–, ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:[ãªã—] ({exclude_reason})")
            else:
                self._log_debug(f"  â†’ {doc_type}: ã‚¹ã‚³ã‚¢:{total_score:.1f}, ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:{combined_keywords}")
                if text_score > 0:
                    self._log_debug(f"    - ãƒ†ã‚­ã‚¹ãƒˆã‚¹ã‚³ã‚¢: {text_score:.1f}")
                if filename_score > 0:
                    self._log_debug(f"    - ãƒ•ã‚¡ã‚¤ãƒ«åã‚¹ã‚³ã‚¢: {filename_score:.1f} Ã— 1.5 = {filename_score * 1.5:.1f}")
            
            # æœ€é«˜ã‚¹ã‚³ã‚¢æ›´æ–°
            if not excluded and total_score > best_score:
                best_score = total_score
                best_match = doc_type
                best_keywords = combined_keywords
                self._log_debug(f"    æ–°ãŸãªæœ€é«˜ã‚¹ã‚³ã‚¢! â†’ {doc_type}")
        
        # ä¿¡é ¼åº¦ã‚’è¨ˆç®—ï¼ˆ0.0-1.0ï¼‰ä¼šè¨ˆæ›¸é¡ç”¨ã«èª¿æ•´
        confidence = min(best_score / 10.0, 1.0)  # ä¼šè¨ˆæ›¸é¡ç”¨ã«é–¾å€¤ã‚’ä¸‹ã’ã‚‹
        
        self._log(f"æœ€çµ‚çµæœ: {best_match}, ã‚¹ã‚³ã‚¢: {best_score:.1f}, ä¿¡é ¼åº¦: {confidence:.2f}")
        
        # åˆ†é¡ã§ããªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆä¼šè¨ˆæ›¸é¡ç”¨ã«é–¾å€¤ã‚’ä¸‹ã’ã‚‹ï¼‰
        if not best_match or confidence < 0.2:  # 0.3 â†’ 0.2 ã«å¤‰æ›´
            best_match = "9999_æœªåˆ†é¡"
            confidence = 0.0
            best_method = "default_fallback"
            self._log(f"ä¿¡é ¼åº¦ä¸è¶³ã«ã‚ˆã‚Šæœªåˆ†é¡ã«å¤‰æ›´")
        
        result = ClassificationResult(
            document_type=best_match,
            confidence=confidence,
            matched_keywords=best_keywords,
            classification_method=best_method,
            debug_steps=debug_steps,
            processing_log=self.processing_log.copy(),
            original_doc_type_code=best_match  # å…ƒã®åˆ†é¡ã‚³ãƒ¼ãƒ‰ã‚’ä¿å­˜
        )
        
        # no_split ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¨­å®šï¼ˆè³‡ç”£ãƒ»å¸³ç¥¨ç³»ï¼‰
        self._set_no_split_metadata(result)
        return result

    def _preprocess_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†"""
        if not text:
            return ""
        
        # ä¸è¦ãªç©ºç™½ãƒ»æ”¹è¡Œã‚’é™¤å»
        cleaned = re.sub(r'\s+', ' ', text)
        
        # å…¨è§’è‹±æ•°å­—ã‚’åŠè§’ã«å¤‰æ›
        cleaned = cleaned.translate(str.maketrans(
            'ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼ºï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½š',
            '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'
        ))
        
        return cleaned.strip()

    def _calculate_score(self, text: str, rules: Dict, source: str = "") -> Tuple[float, List[str]]:
        """åˆ†é¡ãƒ«ãƒ¼ãƒ«ã«åŸºã¥ã„ã¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        score = 0
        matched_keywords = []
        priority = rules.get("priority", 5)
        
        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ï¼ˆå„ªå…ˆï¼‰
        for exclude_keyword in rules.get("exclude_keywords", []):
            if exclude_keyword in text:
                self._log_debug(f"    é™¤å¤–: {source}é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º: '{exclude_keyword}'")
                return 0, []
        
        # å®Œå…¨ä¸€è‡´ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆé«˜ã‚¹ã‚³ã‚¢ï¼‰
        for exact_keyword in rules.get("exact_keywords", []):
            if exact_keyword in text:
                points = priority * 2
                score += points
                matched_keywords.append(exact_keyword)
                self._log_debug(f"    å®Œå…¨ä¸€è‡´: {source}'{exact_keyword}' (+{points})")
        
        # éƒ¨åˆ†ä¸€è‡´ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¸­ã‚¹ã‚³ã‚¢ï¼‰
        for partial_keyword in rules.get("partial_keywords", []):
            if partial_keyword in text:
                points = priority * 1
                score += points
                matched_keywords.append(partial_keyword)
                self._log_debug(f"    éƒ¨åˆ†ä¸€è‡´: {source}'{partial_keyword}' (+{points})")
        
        return score, matched_keywords

    def _calculate_filename_score(self, filename: str, rules: Dict) -> Tuple[float, List[str]]:
        """ãƒ•ã‚¡ã‚¤ãƒ«åã«åŸºã¥ã„ã¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        score = 0
        matched_keywords = []
        priority = rules.get("priority", 5)
        
        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã§ã‚‚é‡è¦ï¼‰
        for exclude_keyword in rules.get("exclude_keywords", []):
            if exclude_keyword in filename:
                self._log_debug(f"    é™¤å¤–: ãƒ•ã‚¡ã‚¤ãƒ«åé™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º: '{exclude_keyword}'")
                return 0, []
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åå°‚ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆ
        filename_keywords = rules.get("filename_keywords", [])
        for keyword in filename_keywords:
            if keyword in filename:
                # ãƒã‚°ä¿®æ­£ä¾é ¼æ›¸: C-2 å¸‚å½¹æ‰€ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã®é‡ã¿ä»˜ã‘å¼·åŒ–
                multiplier = 3.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é‡ã¿ä»˜ã‘
                if keyword == "å¸‚å½¹æ‰€" and "å¸‚ç”ºæ‘ç”³å‘Šæ›¸" in str(rules.get("partial_keywords", [])):
                    multiplier = 9.0  # å¸‚å½¹æ‰€ â†’ ãƒ•ã‚¡ã‚¤ãƒ«åã‚¹ã‚³ã‚¢ Ã— 3.0 ã®é‡ã¿ä»˜ã‘é©ç”¨
                    self._log_debug(f"    å¸‚å½¹æ‰€ãƒ‘ã‚¿ãƒ¼ãƒ³é‡ã¿ä»˜ã‘å¼·åŒ–: å¸‚æ°‘ç¨é–¢é€£ã§Ã—{multiplier}")
                
                points = priority * multiplier
                score += points
                matched_keywords.append(f"[ãƒ•ã‚¡ã‚¤ãƒ«å]{keyword}")
                self._log_debug(f"    ãƒ•ã‚¡ã‚¤ãƒ«åå°‚ç”¨ä¸€è‡´: '{keyword}' (+{points})")
        
        # é€šå¸¸ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚ãƒ•ã‚¡ã‚¤ãƒ«åã§ãƒã‚§ãƒƒã‚¯
        for exact_keyword in rules.get("exact_keywords", []):
            if exact_keyword in filename:
                points = priority * 2
                score += points
                matched_keywords.append(f"[ãƒ•ã‚¡ã‚¤ãƒ«å]{exact_keyword}")
                self._log_debug(f"    ãƒ•ã‚¡ã‚¤ãƒ«åå®Œå…¨ä¸€è‡´: '{exact_keyword}' (+{points})")
        
        return score, matched_keywords

    def classify_with_municipality_info_v5(self, text: str, filename: str, 
                                         prefecture_code: Optional[int] = None,
                                         municipality_code: Optional[int] = None,
                                         municipality_sets: Optional[Dict[int, Dict[str, str]]] = None,
                                         job_context=None) -> ClassificationResult:
        """v5.0 è‡ªæ²»ä½“æƒ…å ±ã‚’è€ƒæ…®ã—ãŸåˆ†é¡ï¼ˆã‚¹ãƒ†ãƒ¼ãƒˆãƒ¬ã‚¹å¯¾å¿œï¼‰"""
        # v5.0 åˆ†é¡å®Ÿè¡Œ
        base_result = self.classify_document_v5(text, filename, job_context)
        
        # æ­£è¦åŒ–å‡¦ç†ã§ãƒ©ãƒ™ãƒ«è§£æ±ºï¼ˆå¿…ãšå®Ÿè¡Œï¼‰
        if municipality_sets:
            print(f"[DEBUG] æ­£è¦åŒ–å‡¦ç†é–‹å§‹: municipality_sets={municipality_sets}")
            # å…ƒã®åˆ†é¡ã‚³ãƒ¼ãƒ‰ã‚’ä¿å­˜ï¼ˆè‡ªæ²»ä½“é©ç”¨å‰ï¼‰
            if base_result.original_doc_type_code is None:
                base_result.original_doc_type_code = base_result.document_type
            
            # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯ï¼šLOCAL_TAXä»¥å¤–ã§ã¯è‡ªæ²»ä½“å¤‰æ›´ç‰ˆã‚’ã‚¹ã‚­ãƒƒãƒ—
            domain = self.code_domain(base_result.document_type)
            if domain != "LOCAL_TAX":
                self._log(f"overlay=SKIPPED(domain={domain})")
                # LOCAL_TAXä»¥å¤–ã§ã¯ä½•ã‚‚ã—ãªã„
            else:
                code, final_label, resolved_set_id = self.normalize_classification(
                    text, filename, base_result.document_type, municipality_sets
                )
                
                if final_label != base_result.document_type:
                    self._log(f"è‡ªæ²»ä½“åä»˜ãã‚³ãƒ¼ãƒ‰ç”Ÿæˆ: {base_result.document_type} â†’ {final_label}")
                    base_result.document_type = final_label
        else:
            print(f"[DEBUG] å¾“æ¥å‡¦ç†å®Ÿè¡Œ: municipality_sets={municipality_sets}")
            # ã‚»ãƒƒãƒˆè¨­å®šãŒãªã„å ´åˆã¯å¾“æ¥å‡¦ç†
            self.current_municipality_sets = municipality_sets or {}
            # å…ƒã®åˆ†é¡ã‚³ãƒ¼ãƒ‰ã‚’ä¿å­˜ï¼ˆè‡ªæ²»ä½“é©ç”¨å‰ï¼‰
            if base_result.original_doc_type_code is None:
                base_result.original_doc_type_code = base_result.document_type
            
            # ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯ï¼šLOCAL_TAXä»¥å¤–ã§ã¯è‡ªæ²»ä½“å¤‰æ›´ç‰ˆã‚’ã‚¹ã‚­ãƒƒãƒ—
            domain = self.code_domain(base_result.document_type)
            if domain != "LOCAL_TAX":
                self._log(f"overlay=SKIPPED(domain={domain})")
                # LOCAL_TAXä»¥å¤–ã§ã¯ä½•ã‚‚ã—ãªã„
            else:
                final_code = self._apply_municipality_numbering(
                    base_result.document_type, 
                    prefecture_code, 
                    municipality_code,
                    text,
                    filename
                )
                
                if final_code != base_result.document_type:
                    self._log(f"è‡ªæ²»ä½“åä»˜ãã‚³ãƒ¼ãƒ‰ç”Ÿæˆ: {base_result.document_type} â†’ {final_code}")
                    base_result.document_type = final_code
        
        return base_result

    def _apply_municipality_numbering(self, document_type: str, 
                                    prefecture_code: Optional[int] = None,
                                    municipality_code: Optional[int] = None,
                                    text_content: str = "",
                                    filename: str = "") -> str:
        """è‡ªæ²»ä½“é€£ç•ªã®é©ç”¨ï¼ˆä¿®æ­£ç‰ˆï¼šå›ºå®šç•ªå·ã‚’å³æ ¼ã«ç®¡ç†ï¼‰"""
        self._log_debug(f"è‡ªæ²»ä½“é€£ç•ªé©ç”¨ãƒã‚§ãƒƒã‚¯: {document_type}, éƒ½é“åºœçœŒ={prefecture_code}, å¸‚ç”ºæ‘={municipality_code}")
        
        # ä¿®æ­£1: å›ºå®šç•ªå·ã¯é€£ç•ªé©ç”¨é™¤å¤–ï¼ˆé‡è¦ãªä¿®æ­£ï¼‰  
        # ä¿®æ­£æŒ‡ç¤ºæ›¸: ç´ä»˜æƒ…å ±ã¯å›ºå®šã€å—ä¿¡é€šçŸ¥ã¯é€£ç•ªå¯¾å¿œ
        FIXED_NUMBERS = {
            "0003_å—ä¿¡é€šçŸ¥",     # æ³•äººç¨å—ä¿¡é€šçŸ¥ã¯å›ºå®š
            "0004_ç´ä»˜æƒ…å ±",     # æ³•äººç¨ç´ä»˜æƒ…å ±ã¯å›ºå®š
            "3003_å—ä¿¡é€šçŸ¥",     # æ¶ˆè²»ç¨å—ä¿¡é€šçŸ¥ã¯å›ºå®š
            "3004_ç´ä»˜æƒ…å ±",     # æ¶ˆè²»ç¨ç´ä»˜æƒ…å ±ã¯å›ºå®š
            "1004_ç´ä»˜æƒ…å ±",     # éƒ½é“åºœçœŒç´ä»˜æƒ…å ±ã¯å›ºå®š
            "2004_ç´ä»˜æƒ…å ±"      # å¸‚ç”ºæ‘ç´ä»˜æƒ…å ±ã¯å›ºå®š
            # æ³¨æ„: 1003_å—ä¿¡é€šçŸ¥ã¨2003_å—ä¿¡é€šçŸ¥ã¯é€£ç•ªé©ç”¨å¯¾è±¡ã®ãŸã‚é™¤å¤–
        }
        
        if document_type in FIXED_NUMBERS:
            self._log_debug(f"å›ºå®šç•ªå·ã®ãŸã‚é€£ç•ªé©ç”¨é™¤å¤–: {document_type}")
            return document_type
        
        # é€£ç•ªé©ç”¨: ç”³å‘Šæ›¸ç³»çµ±ã¸ã®è‡ªæ²»ä½“é€£ç•ªã®é©ç”¨
        # éƒ½é“åºœçœŒç”³å‘Šæ›¸ï¼ˆ1001ç³»çµ±ï¼‰
        if document_type == "1001_éƒ½é“åºœçœŒ_éƒ½é“åºœçœŒç”³å‘Šæ›¸":
            if prefecture_code:
                prefecture_name = self._get_prefecture_name(prefecture_code)
                
                # v5.3.4 prefecture-specific code resolution
                resolved_code = self.resolve_local_tax_class(document_type, prefecture_name)
                final_code = f"{prefecture_code}_{prefecture_name}_éƒ½é“åºœçœŒç”³å‘Šæ›¸"
                
                # Use resolved code if different
                if resolved_code != document_type and "_" in resolved_code:
                    code_part = resolved_code.split("_")[0]
                    final_code = f"{code_part}_{prefecture_name}_éƒ½é“åºœçœŒç”³å‘Šæ›¸"
                
                self._log_debug(f"éƒ½é“åºœçœŒç”³å‘Šæ›¸é€£ç•ªé©ç”¨: {document_type} â†’ {final_code}")
                return final_code
        
        # å¸‚ç”ºæ‘ç”³å‘Šæ›¸ï¼ˆ2001ç³»çµ±ï¼‰
        elif document_type.startswith("2") and document_type.endswith("_å¸‚ç”ºæ‘_å¸‚ç”ºæ‘ç”³å‘Šæ›¸"):
            if municipality_code:
                # ã‚»ãƒƒãƒˆè¨­å®šæƒ…å ±ã®ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
                if hasattr(self, 'current_municipality_sets'):
                    self._log_debug(f"ã‚»ãƒƒãƒˆè¨­å®šæƒ…å ±åˆ©ç”¨å¯èƒ½: {self.current_municipality_sets}")
                else:
                    self._log_debug(f"ã‚»ãƒƒãƒˆè¨­å®šæƒ…å ±ãªã—: current_municipality_setsãŒæœªè¨­å®š")
                
                municipality_name = self._get_municipality_name(municipality_code)
                final_code = f"{municipality_code}_{municipality_name}_å¸‚ç”ºæ‘ç”³å‘Šæ›¸"
                print(f"[DEBUG] å¸‚ç”ºæ‘ç”³å‘Šæ›¸é€£ç•ªé©ç”¨: {document_type} â†’ {final_code}")
                self._log_debug(f"å¸‚ç”ºæ‘ç”³å‘Šæ›¸é€£ç•ªé©ç”¨: {document_type} â†’ {final_code}")
                return final_code
        
        # ä¿®æ­£æŒ‡ç¤ºæ›¸: ä¿®æ­£5 - éƒ½é“åºœçœŒå—ä¿¡é€šçŸ¥ã®é€£ç•ªå¯¾å¿œï¼ˆOCRãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥èª­ã¿å–ã‚Šï¼‰
        elif document_type == "1003_å—ä¿¡é€šçŸ¥":
            self._log_debug(f"[OCR DEBUG] éƒ½é“åºœçœŒå—ä¿¡é€šçŸ¥å‡¦ç†é–‹å§‹: {document_type}")
            self._log_debug(f"[OCR DEBUG] text_content length: {len(text_content) if text_content else 0}")
            self._log_debug(f"[OCR DEBUG] has current_municipality_sets: {hasattr(self, 'current_municipality_sets')}")
            
            # OCRãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å®Ÿéš›ã®éƒ½é“åºœçœŒã‚’èª­ã¿å–ã‚Šã€ã‚»ãƒƒãƒˆè¨­å®šã¨ç…§åˆ
            if hasattr(self, 'current_municipality_sets') and text_content:
                self._log_debug(f"[OCR DEBUG] ã‚»ãƒƒãƒˆè¨­å®š: {self.current_municipality_sets}")
                detected_prefecture = self._extract_prefecture_from_receipt_text(text_content, filename)
                self._log_debug(f"[OCR DEBUG] æ¤œå‡ºã•ã‚ŒãŸéƒ½é“åºœçœŒ: {detected_prefecture}")
                
                if detected_prefecture:
                    # ã‚»ãƒƒãƒˆè¨­å®šã‹ã‚‰è©²å½“ã™ã‚‹ã‚»ãƒƒãƒˆIDã‚’ç‰¹å®š
                    for set_id, info in self.current_municipality_sets.items():
                        self._log_debug(f"[OCR DEBUG] ã‚»ãƒƒãƒˆ{set_id}ãƒã‚§ãƒƒã‚¯: {info.get('prefecture')} == {detected_prefecture}")
                        if info.get("prefecture") == detected_prefecture:
                            receipt_code = 1003 + (set_id - 1) * 10
                            self._log_debug(f"éƒ½é“åºœçœŒå—ä¿¡é€šçŸ¥OCRæ¤œå‡º: {detected_prefecture} â†’ ã‚»ãƒƒãƒˆ{set_id} â†’ {receipt_code}_å—ä¿¡é€šçŸ¥")
                            return f"{receipt_code}_å—ä¿¡é€šçŸ¥"
                    
                    self._log_debug(f"éƒ½é“åºœçœŒå—ä¿¡é€šçŸ¥: OCRã§æ¤œå‡ºã—ãŸ'{detected_prefecture}'ãŒã‚»ãƒƒãƒˆè¨­å®šã«ã‚ã‚Šã¾ã›ã‚“")
            else:
                self._log_debug(f"[OCR DEBUG] OCRå‡¦ç†ã‚¹ã‚­ãƒƒãƒ— - ã‚»ãƒƒãƒˆè¨­å®šã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆãªã—")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¾“æ¥ã®æ–¹å¼
            if prefecture_code:
                self._log_debug(f"[OCR DEBUG] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†: prefecture_code={prefecture_code}")
                receipt_code = 1003 + ((prefecture_code - 1001) // 10) * 10
                return f"{receipt_code}_å—ä¿¡é€šçŸ¥"
        
        # å¸‚ç”ºæ‘å—ä¿¡é€šçŸ¥ï¼ˆ2003ç³»çµ±ï¼‰ã®é€£ç•ªå¯¾å¿œï¼ˆOCRãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥èª­ã¿å–ã‚Šï¼‰
        elif document_type == "2003_å—ä¿¡é€šçŸ¥":
            self._log_debug(f"[OCR DEBUG] å¸‚ç”ºæ‘å—ä¿¡é€šçŸ¥å‡¦ç†é–‹å§‹: {document_type}")
            self._log_debug(f"[OCR DEBUG] text_content length: {len(text_content) if text_content else 0}")
            
            # OCRãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å®Ÿéš›ã®å¸‚ç”ºæ‘ã‚’èª­ã¿å–ã‚Šã€ã‚»ãƒƒãƒˆè¨­å®šã¨ç…§åˆ
            if hasattr(self, 'current_municipality_sets') and text_content:
                detected_prefecture, detected_city = self._extract_municipality_from_receipt_text(text_content, filename)
                self._log_debug(f"[OCR DEBUG] æ¤œå‡ºã•ã‚ŒãŸå¸‚ç”ºæ‘: {detected_prefecture} {detected_city}")
                
                if detected_prefecture and detected_city:
                    # ã‚»ãƒƒãƒˆè¨­å®šã‹ã‚‰è©²å½“ã™ã‚‹ã‚»ãƒƒãƒˆIDã‚’ç‰¹å®š
                    for set_id, info in self.current_municipality_sets.items():
                        self._log_debug(f"[OCR DEBUG] ã‚»ãƒƒãƒˆ{set_id}ãƒã‚§ãƒƒã‚¯: {info.get('prefecture')}{info.get('city')} == {detected_prefecture}{detected_city}")
                        if (info.get("prefecture") == detected_prefecture and 
                            info.get("city") == detected_city):
                            receipt_code = 2003 + (set_id - 1) * 10
                            self._log_debug(f"å¸‚ç”ºæ‘å—ä¿¡é€šçŸ¥OCRæ¤œå‡º: {detected_prefecture}{detected_city} â†’ ã‚»ãƒƒãƒˆ{set_id} â†’ {receipt_code}_å—ä¿¡é€šçŸ¥")
                            return f"{receipt_code}_å—ä¿¡é€šçŸ¥"
                    
                    self._log_debug(f"å¸‚ç”ºæ‘å—ä¿¡é€šçŸ¥: OCRã§æ¤œå‡ºã—ãŸ'{detected_prefecture}{detected_city}'ãŒã‚»ãƒƒãƒˆè¨­å®šã«ã‚ã‚Šã¾ã›ã‚“")
            else:
                self._log_debug(f"[OCR DEBUG] OCRå‡¦ç†ã‚¹ã‚­ãƒƒãƒ— - ã‚»ãƒƒãƒˆè¨­å®šã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆãªã—")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¾“æ¥ã®æ–¹å¼
            if municipality_code:
                self._log_debug(f"[OCR DEBUG] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†: municipality_code={municipality_code}")
                receipt_code = 2003 + ((municipality_code - 2001) // 10) * 10
                return f"{receipt_code}_å—ä¿¡é€šçŸ¥"
        
        self._log_debug(f"è‡ªæ²»ä½“é€£ç•ªé©ç”¨ãªã—: {document_type}")
        return document_type

    def _extract_prefecture_from_receipt_text(self, text_content: str, filename: str) -> Optional[str]:
        """å—ä¿¡é€šçŸ¥ã®OCRãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éƒ½é“åºœçœŒã‚’æŠ½å‡º"""
        # å—ä¿¡é€šçŸ¥ã«å«ã¾ã‚Œã‚‹éƒ½é“åºœçœŒãƒ‘ã‚¿ãƒ¼ãƒ³
        prefecture_patterns = [
            r'(æ±äº¬éƒ½)',
            r'(å¤§é˜ªåºœ)',
            r'(æ„›çŸ¥çœŒ)',
            r'(ç¦å²¡çœŒ)',
            r'(åŒ—æµ·é“)',
            r'([^çœŒå¸‚åŒºç”ºæ‘]+çœŒ)',  # ãã®ä»–ã®çœŒ
            r'([^åºœçœŒå¸‚åŒºç”ºæ‘]+åºœ)'   # ãã®ä»–ã®åºœ
        ]
        
        for pattern in prefecture_patterns:
            import re
            match = re.search(pattern, text_content)
            if match:
                prefecture = match.group(1)
                self._log_debug(f"å—ä¿¡é€šçŸ¥ã‹ã‚‰éƒ½é“åºœçœŒæ¤œå‡º: {prefecture}")
                return prefecture
        
        self._log_debug(f"å—ä¿¡é€šçŸ¥ã‹ã‚‰éƒ½é“åºœçœŒæ¤œå‡ºãªã—: {filename}")
        return None

    def _extract_municipality_from_receipt_text(self, text_content: str, filename: str) -> Tuple[Optional[str], Optional[str]]:
        """å—ä¿¡é€šçŸ¥ã®OCRãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éƒ½é“åºœçœŒã¨å¸‚ç”ºæ‘ã‚’æŠ½å‡º"""
        # å¸‚ç”ºæ‘å—ä¿¡é€šçŸ¥ã«å«ã¾ã‚Œã‚‹éƒ½é“åºœçœŒãƒ»å¸‚ç”ºæ‘ãƒ‘ã‚¿ãƒ¼ãƒ³
        municipality_patterns = [
            r'(æ„›çŸ¥çœŒ).*(è’²éƒ¡å¸‚)',
            r'(ç¦å²¡çœŒ).*(ç¦å²¡å¸‚)',
            r'([^çœŒå¸‚åŒºç”ºæ‘]+çœŒ).+?([^çœŒå¸‚åŒºç”ºæ‘]+å¸‚)',
            r'([^åºœçœŒå¸‚åŒºç”ºæ‘]+åºœ).+?([^åºœçœŒå¸‚åŒºç”ºæ‘]+å¸‚)'
        ]
        
        for pattern in municipality_patterns:
            import re
            match = re.search(pattern, text_content)
            if match:
                prefecture = match.group(1)
                city = match.group(2)
                self._log_debug(f"å—ä¿¡é€šçŸ¥ã‹ã‚‰å¸‚ç”ºæ‘æ¤œå‡º: {prefecture}{city}")
                return prefecture, city
        
        self._log_debug(f"å—ä¿¡é€šçŸ¥ã‹ã‚‰å¸‚ç”ºæ‘æ¤œå‡ºãªã—: {filename}")
        return None, None

    def build_order_maps(self, set_settings: Dict[int, Dict[str, str]]) -> Tuple[Dict[int, int], Dict[int, int]]:
        """ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¬ã‚¹é€£ç•ªãƒãƒƒãƒ—ã‚’æ§‹ç¯‰
        
        Args:
            set_settings: ã‚»ãƒƒãƒˆè¨­å®šè¾æ›¸ {set_id: {"prefecture": str, "city": str}}
            
        Returns:
            Tuple[pref_order_map, city_order_map]
            pref_order_map: {set_id: prefecture_code}
            city_order_map: {set_id: municipality_code}
        """
        # æ±äº¬éƒ½ãƒã‚§ãƒƒã‚¯ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
        tokyo_set_id = None
        for set_id, info in set_settings.items():
            if info.get("prefecture") == "æ±äº¬éƒ½":
                tokyo_set_id = set_id
                # æ±äº¬éƒ½ã¯å¿…ãšã‚»ãƒƒãƒˆ1ã§ãªã‘ã‚Œã°ãªã‚‰ãªã„
                if set_id != 1:
                    raise ValueError(f"æ±äº¬éƒ½ã¯å¿…ãšã‚»ãƒƒãƒˆ1ã«å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ç¾åœ¨ã®ä½ç½®: ã‚»ãƒƒãƒˆ{set_id}")
                # æ±äº¬éƒ½ã«cityãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚¨ãƒ©ãƒ¼
                if info.get("city", "").strip():
                    raise ValueError(f"æ±äº¬éƒ½ï¼ˆã‚»ãƒƒãƒˆ{set_id}ï¼‰ã«cityãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™: {info.get('city')}")
                break
        
        # éƒ½é“åºœçœŒé€£ç•ªãƒãƒƒãƒ—
        pref_order_map = {}
        sorted_set_ids = sorted(set_settings.keys())
        
        if tokyo_set_id is not None:
            # æ±äº¬éƒ½ãŒã‚ã‚‹å ´åˆï¼šè«–ç†çš„ã«å…ˆé ­ã«ç§»å‹•
            ordered_sets = [tokyo_set_id] + [sid for sid in sorted_set_ids if sid != tokyo_set_id]
        else:
            # æ±äº¬éƒ½ãŒãªã„å ´åˆï¼šå…¥åŠ›é †ã®ã¾ã¾
            ordered_sets = sorted_set_ids
        
        for rank, set_id in enumerate(ordered_sets):
            pref_order_map[set_id] = 1001 + rank * 10
        
        # å¸‚ç”ºæ‘é€£ç•ªãƒãƒƒãƒ—: cityãŒç©ºã§ãªã„ã‚»ãƒƒãƒˆã®ã¿ã‚’é †åºåŒ–
        city_sets = []
        for set_id in sorted(set_settings.keys()):
            city = set_settings[set_id].get("city", "").strip()
            if city:  # cityãŒç©ºã§ãªã„å ´åˆã®ã¿
                city_sets.append(set_id)
        
        city_order_map = {}
        for rank, set_id in enumerate(city_sets):
            city_order_map[set_id] = 2001 + rank * 10
            
        return pref_order_map, city_order_map
    
    def _get_city_order_from_code(self, municipality_code: int) -> int:
        """å¸‚ç”ºæ‘ã‚³ãƒ¼ãƒ‰ã‹ã‚‰é †åºã‚’å–å¾—ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼é–¢æ•° - å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰"""
        code_to_order = {
            2001: 1,  # 1ç•ªç›®ã®å¸‚ç”ºæ‘
            2011: 2,  # 2ç•ªç›®ã®å¸‚ç”ºæ‘
            2021: 3,  # 3ç•ªç›®ã®å¸‚ç”ºæ‘
            2031: 4,  # 4ç•ªç›®ã®å¸‚ç”ºæ‘
            2041: 5   # 5ç•ªç›®ã®å¸‚ç”ºæ‘
        }
        return code_to_order.get(municipality_code, 1)

    def _get_prefecture_name(self, prefecture_code: int) -> str:
        """éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ã‹ã‚‰å®Ÿéš›ã®éƒ½é“åºœçœŒåã‚’å–å¾—"""
        # ã‚»ãƒƒãƒˆè¨­å®šã‹ã‚‰éƒ½é“åºœçœŒåã‚’å–å¾—
        if hasattr(self, 'current_municipality_sets') and self.current_municipality_sets:
            # prefecture_codeã‹ã‚‰é€†ç®—ã—ã¦ã‚»ãƒƒãƒˆç•ªå·ã‚’å–å¾—
            set_id = self._get_set_id_from_prefecture_code(prefecture_code)
            if set_id and set_id in self.current_municipality_sets:
                return self.current_municipality_sets[set_id].get('prefecture', 'éƒ½é“åºœçœŒ')
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚³ãƒ¼ãƒ‰ã‹ã‚‰æ¨æ¸¬
        code_to_name = {
            1001: 'æ±äº¬éƒ½',
            1011: 'æ„›çŸ¥çœŒ', 
            1021: 'ç¦å²¡çœŒ',
            1031: 'å¤§é˜ªåºœ',
            1041: 'ç¥å¥ˆå·çœŒ'
        }
        return code_to_name.get(prefecture_code, 'éƒ½é“åºœçœŒ')

    def _get_municipality_name(self, municipality_code: int) -> str:
        """å¸‚ç”ºæ‘ã‚³ãƒ¼ãƒ‰ã‹ã‚‰å®Ÿéš›ã®å¸‚ç”ºæ‘åã‚’å–å¾—"""
        # ã‚»ãƒƒãƒˆè¨­å®šã‹ã‚‰å¸‚ç”ºæ‘åã‚’å–å¾—
        if hasattr(self, 'current_municipality_sets') and self.current_municipality_sets:
            # municipality_codeã‹ã‚‰é€†ç®—ã—ã¦ã‚»ãƒƒãƒˆç•ªå·ã‚’å–å¾—
            set_id = self._get_set_id_from_municipality_code(municipality_code)
            if set_id and set_id in self.current_municipality_sets:
                pref = self.current_municipality_sets[set_id].get('prefecture', '')
                city = self.current_municipality_sets[set_id].get('city', '')
                if pref and city:
                    return f"{pref}{city}"
                elif pref:
                    return pref
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚³ãƒ¼ãƒ‰ã‹ã‚‰æ¨æ¸¬
        code_to_name = {
            2001: 'æ„›çŸ¥çœŒè’²éƒ¡å¸‚',
            2011: 'ç¦å²¡çœŒç¦å²¡å¸‚',
            2021: 'å¤§é˜ªå¸‚',
            2031: 'æ¨ªæµœå¸‚',
            2041: 'åå¤å±‹å¸‚'
        }
        return code_to_name.get(municipality_code, 'å¸‚ç”ºæ‘')

    def _get_set_id_from_prefecture_code(self, prefecture_code: int) -> Optional[int]:
        """éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã‚»ãƒƒãƒˆç•ªå·ã‚’å–å¾—"""
        # 1001, 1011, 1021, 1031, 1041 -> 1, 2, 3, 4, 5
        if prefecture_code >= 1001 and prefecture_code <= 1041 and (prefecture_code - 1001) % 10 == 0:
            return ((prefecture_code - 1001) // 10) + 1
        return None

    def _get_set_id_from_municipality_code(self, municipality_code: int) -> Optional[int]:
        """å¸‚ç”ºæ‘ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã‚»ãƒƒãƒˆç•ªå·ã‚’å–å¾—"""
        # 2001, 2011, 2021, 2031, 2041 -> ã‚»ãƒƒãƒˆç•ªå·ã‚’æ±ºå®š
        # æ³¨æ„: å¸‚ç”ºæ‘ã‚³ãƒ¼ãƒ‰ã¯æ±äº¬éƒ½ã‚’é™¤ã„ãŸé †åºãªã®ã§ã€å®Ÿéš›ã®ã‚»ãƒƒãƒˆç•ªå·ã¯+1ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚‹
        if municipality_code >= 2001 and municipality_code <= 2041 and (municipality_code - 2001) % 10 == 0:
            # å¸‚ç”ºæ‘ã¯æ±äº¬éƒ½ã‚’é™¤ã„ãŸé †åºãªã®ã§ã€å®Ÿéš›ã«ã©ã®ã‚»ãƒƒãƒˆã‹ã‚’ç‰¹å®šã™ã‚‹ãŸã‚ã«ã¯
            # current_municipality_setsã‚’ç¢ºèªã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
            if hasattr(self, 'current_municipality_sets') and self.current_municipality_sets:
                city_sets = [(set_id, info) for set_id, info in self.current_municipality_sets.items() 
                            if info.get('city', '').strip()]
                city_sets.sort(key=lambda x: x[0])  # ã‚»ãƒƒãƒˆç•ªå·é †
                
                order = ((municipality_code - 2001) // 10) + 1
                if order <= len(city_sets):
                    return city_sets[order - 1][0]
        return None

    def _resolve_document_label_stateless(self, document_type: str, extracted_text: str, 
                                         filename: str, set_settings: Dict[int, Dict[str, str]]) -> str:
        """ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¬ã‚¹ãªæ–‡æ›¸ãƒ©ãƒ™ãƒ«è§£æ±ºï¼ˆä¸æ•´åˆæ¤œå‡ºæ©Ÿèƒ½ä»˜ãï¼‰"""
        try:
            # 1. é€£ç•ªãƒãƒƒãƒ—ã‚’æ§‹ç¯‰
            pref_order_map, city_order_map = self.build_order_maps(set_settings)
            
            # 2. ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è‡ªæ²»ä½“æƒ…å ±ã‚’æŠ½å‡ºï¼ˆæ¤œè¨¼ç”¨ï¼‰
            extracted_pref, extracted_city = self._extract_pref_city_from_text(extracted_text, filename)
            
            # 3. æ–‡æ›¸åˆ†é¡ã‹ã‚‰è‡ªæ²»ä½“ã‚³ãƒ¼ãƒ‰ã‚’æ±ºå®š
            detected_set = self._detect_municipality_set_from_text(extracted_text, filename, set_settings)
            if not detected_set:
                self._log_inconsistency(filename, extracted_pref or "ä¸æ˜", extracted_city or "ä¸æ˜", 
                                       "æœªåˆ†é¡", "æœªåˆ†é¡", "ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è‡ªæ²»ä½“ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return document_type
            
            # 4. è‡ªæ²»ä½“ç¨®åˆ¥åˆ¤å®šï¼ˆéƒ½é“åºœçœŒç¨ vs å¸‚æ°‘ç¨ï¼‰
            is_prefecture_tax = self._is_prefecture_tax_document(document_type)
            is_municipal_tax = self._is_municipal_tax_document(document_type)
            
            if is_prefecture_tax:
                # éƒ½é“åºœçœŒç¨ã®å ´åˆ
                if detected_set in pref_order_map:
                    pref_code = pref_order_map[detected_set]
                    resolved_pref = set_settings[detected_set]["prefecture"]
                    resolved_city = ""
                    
                    # ä¸æ•´åˆæ¤œå‡º
                    if extracted_pref and extracted_pref != resolved_pref:
                        self._log_inconsistency(filename, extracted_pref, extracted_city or "", 
                                              resolved_pref, resolved_city, "éƒ½é“åºœçœŒåã®ä¸ä¸€è‡´")
                    
                    return f"{pref_code}_{resolved_pref}_éƒ½é“åºœçœŒç”³å‘Šæ›¸"
                    
            elif is_municipal_tax:
                # å¸‚æ°‘ç¨ã®å ´åˆ
                if detected_set in city_order_map:
                    city_code = city_order_map[detected_set]
                    resolved_pref = set_settings[detected_set]["prefecture"]
                    resolved_city = set_settings[detected_set]["city"]
                    
                    # ä¸æ•´åˆæ¤œå‡º
                    if extracted_pref and extracted_pref != resolved_pref:
                        self._log_inconsistency(filename, extracted_pref, extracted_city or "", 
                                              resolved_pref, resolved_city, "éƒ½é“åºœçœŒåã®ä¸ä¸€è‡´")
                    if extracted_city and extracted_city != resolved_city:
                        self._log_inconsistency(filename, extracted_pref or "", extracted_city, 
                                              resolved_pref, resolved_city, "å¸‚ç”ºæ‘åã®ä¸ä¸€è‡´")
                    
                    return f"{city_code}_{resolved_pref}{resolved_city}_å¸‚ç”ºæ‘ç”³å‘Šæ›¸"
            
            return document_type
            
        except Exception as e:
            self._log_debug(f"ãƒ©ãƒ™ãƒ«è§£æ±ºã‚¨ãƒ©ãƒ¼: {e}")
            return document_type
    
    def _extract_pref_city_from_text(self, text: str, filename: str) -> Tuple[Optional[str], Optional[str]]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éƒ½é“åºœçœŒãƒ»å¸‚ç”ºæ‘åã‚’æŠ½å‡ºï¼ˆæ¤œè¨¼ç”¨ï¼‰"""
        combined_text = f"{text} {filename}"
        
        # éƒ½é“åºœçœŒãƒ‘ã‚¿ãƒ¼ãƒ³
        pref_patterns = [
            (r'æ±äº¬éƒ½', 'æ±äº¬éƒ½'),
            (r'æ„›çŸ¥çœŒ', 'æ„›çŸ¥çœŒ'),
            (r'ç¦å²¡çœŒ', 'ç¦å²¡çœŒ')
        ]
        
        # å¸‚ç”ºæ‘ãƒ‘ã‚¿ãƒ¼ãƒ³  
        city_patterns = [
            (r'è’²éƒ¡å¸‚', 'è’²éƒ¡å¸‚'),
            (r'ç¦å²¡å¸‚', 'ç¦å²¡å¸‚')
        ]
        
        extracted_pref = None
        extracted_city = None
        
        for pattern, name in pref_patterns:
            if re.search(pattern, combined_text):
                extracted_pref = name
                break
                
        for pattern, name in city_patterns:
            if re.search(pattern, combined_text):
                extracted_city = name
                break
        
        return extracted_pref, extracted_city
    
    def _detect_municipality_set_from_text(self, text: str, filename: str, 
                                           set_settings: Dict[int, Dict[str, str]]) -> Optional[int]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è‡ªæ²»ä½“ã‚»ãƒƒãƒˆã‚’æ¤œå‡ºï¼ˆå‹•çš„ï¼‰"""
        combined_text = f"{text} {filename}".lower()
        
        # set_settingsãƒ™ãƒ¼ã‚¹ã§æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‹•çš„ç”Ÿæˆ
        for set_id, info in set_settings.items():
            prefecture = info.get("prefecture", "")
            city = info.get("city", "")
            
            # éƒ½é“åºœçœŒåã§ã®æ¤œå‡º
            if prefecture and prefecture.lower() in combined_text:
                return set_id
            
            # å¸‚ç”ºæ‘åã§ã®æ¤œå‡ºï¼ˆå¸‚å½¹æ‰€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚å«ã‚€ï¼‰
            if city and city.lower() in combined_text:
                return set_id
                
            # å¸‚å½¹æ‰€ãƒ‘ã‚¿ãƒ¼ãƒ³
            if city and f"{city}å½¹æ‰€".lower() in combined_text:
                return set_id
        return None
    
    def resolve_set_id_from_text(self, text: str, filename: str, set_settings: Dict[int, Dict[str, str]], 
                                doc_kind: str) -> Optional[int]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è‡ªæ²»ä½“ã‚»ãƒƒãƒˆIDã‚’è§£æ±ºï¼ˆå¼·ã„æ‰‹ãŒã‹ã‚Šã‹ã‚‰é †ã«åˆ¤å®šï¼‰"""
        combined_text = f"{text} {filename}"
        
        if doc_kind == "pref":
            # çœŒç¨ã®å ´åˆï¼šçœŒç¨äº‹å‹™æ‰€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å„ªå…ˆæ¤œç´¢
            patterns = [
                (r'æ±äº¬éƒ½æ¸¯éƒ½ç¨äº‹å‹™æ‰€', lambda _: self._find_set_by_pref(set_settings, "æ±äº¬éƒ½")),
                (r'(\w+çœŒ).*?çœŒç¨äº‹å‹™æ‰€', lambda m: self._find_set_by_pref(set_settings, m.group(1))),
                (r'(\w+çœŒ)\s*ç¨äº‹å‹™æ‰€', lambda m: self._find_set_by_pref(set_settings, m.group(1))),
                (r'æ±äº¬éƒ½', lambda _: self._find_set_by_pref(set_settings, "æ±äº¬éƒ½")),
            ]
            
        elif doc_kind == "city":
            # å¸‚ç¨ã®å ´åˆï¼šå¸‚å½¹æ‰€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å„ªå…ˆæ¤œç´¢
            patterns = [
                (r'(\w+å¸‚)å½¹æ‰€', lambda m: self._find_set_by_city(set_settings, m.group(1))),
                (r'(\w+å¸‚)é•·', lambda m: self._find_set_by_city(set_settings, m.group(1))),
                (r'å½“è©²å¸‚ç”ºæ‘.*?(\w+çœŒ)', lambda m: self._find_set_by_pref(set_settings, m.group(1))),
            ]
        else:
            return None
            
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã§æ¤œç´¢
        import re
        for pattern, resolver in patterns:
            match = re.search(pattern, combined_text)
            if match:
                result = resolver(match)
                if result:
                    return result
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šä¸€èˆ¬çš„ãªéƒ½é“åºœçœŒãƒ»å¸‚ç”ºæ‘åæ¤œç´¢
        for set_id, info in set_settings.items():
            pref = info.get("prefecture", "")
            city = info.get("city", "")
            
            if doc_kind == "pref" and pref and pref in combined_text:
                return set_id
            elif doc_kind == "city" and city and city in combined_text:
                return set_id
                
        return None
    
    def _find_set_by_pref(self, set_settings: Dict[int, Dict[str, str]], target_pref: str) -> Optional[int]:
        """éƒ½é“åºœçœŒåã‹ã‚‰ã‚»ãƒƒãƒˆIDã‚’æ¤œç´¢"""
        for set_id, info in set_settings.items():
            if info.get("prefecture") == target_pref:
                return set_id
        return None
    
    def _find_set_by_city(self, set_settings: Dict[int, Dict[str, str]], target_city: str) -> Optional[int]:
        """å¸‚ç”ºæ‘åã‹ã‚‰ã‚»ãƒƒãƒˆIDã‚’æ¤œç´¢"""
        for set_id, info in set_settings.items():
            if info.get("city") == target_city:
                return set_id
        return None
    
    def _is_prefecture_tax_document(self, document_type: str) -> bool:
        """éƒ½é“åºœçœŒç¨æ–‡æ›¸ã‹ã©ã†ã‹åˆ¤å®š"""
        prefecture_patterns = ['éƒ½é“åºœçœŒ', 'çœŒç¨', 'éƒ½ç¨', 'é“ç¨', 'åºœç¨']
        return any(pattern in document_type for pattern in prefecture_patterns)
    
    def _is_municipal_tax_document(self, document_type: str) -> bool:
        """å¸‚æ°‘ç¨æ–‡æ›¸ã‹ã©ã†ã‹åˆ¤å®š"""
        municipal_patterns = ['å¸‚æ°‘ç¨', 'å¸‚ç”ºæ‘']
        return any(pattern in document_type for pattern in municipal_patterns)
    
    def _log_inconsistency(self, filename: str, extracted_pref: str, extracted_city: str,
                          resolved_pref: str, resolved_city: str, reason: str):
        """ä¸æ•´åˆãƒ­ã‚°ã®è¨˜éŒ²"""
        import csv
        import os
        
        log_entry = {
            'filename': filename,
            'extracted_pref': extracted_pref,
            'extracted_city': extracted_city,
            'resolved_pref': resolved_pref,
            'resolved_city': resolved_city,
            'reason': reason
        }
        
        # CSVãƒ­ã‚°å‡ºåŠ›
        log_file = 'municipality_inconsistency.csv'
        file_exists = os.path.isfile(log_file)
        
        with open(log_file, 'a', encoding='utf-8', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=log_entry.keys())
            if not file_exists:
                writer.writeheader()
            writer.writerow(log_entry)
        
        # WARNãƒ­ã‚°ã‚‚å‡ºåŠ›
        self._log_debug(f"[WARN] è‡ªæ²»ä½“åä¸æ•´åˆ: {filename} - {reason}")
        print(f"[WARN] è‡ªæ²»ä½“åä¸æ•´åˆ: {filename} - {reason}")
    
    def normalize_classification(self, text: str, filename: str, template_id: str, 
                                set_settings: Dict[int, Dict[str, str]]) -> Tuple[int, str, int]:
        """v5.1ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆIDã‚’æ­£è¦åŒ–ã—ã¦æœ€çµ‚ãƒ©ãƒ™ãƒ«ã‚’ç”Ÿæˆ"""
        print(f"[INFO] æ­£è¦åŒ–å‡¦ç†é–‹å§‹: template_id={template_id}")
        
        # ä¿®æ­£: FIXED_NUMBERSç¢ºèª - ç´ä»˜æƒ…å ±ã¯å›ºå®šã€å—ä¿¡é€šçŸ¥ã¯é€£ç•ªé©ç”¨å¾Œã¯å›ºå®šæ‰±ã„
        FIXED_NUMBERS = {
            "0003_å—ä¿¡é€šçŸ¥",     # æ³•äººç¨å—ä¿¡é€šçŸ¥ã¯å›ºå®š
            "0004_ç´ä»˜æƒ…å ±",     # æ³•äººç¨ç´ä»˜æƒ…å ±ã¯å›ºå®š
            "3003_å—ä¿¡é€šçŸ¥",     # æ¶ˆè²»ç¨å—ä¿¡é€šçŸ¥ã¯å›ºå®š
            "3004_ç´ä»˜æƒ…å ±",     # æ¶ˆè²»ç¨ç´ä»˜æƒ…å ±ã¯å›ºå®š
            "1004_ç´ä»˜æƒ…å ±",     # éƒ½é“åºœçœŒç´ä»˜æƒ…å ±ã¯å›ºå®š
            "2004_ç´ä»˜æƒ…å ±",     # å¸‚ç”ºæ‘ç´ä»˜æƒ…å ±ã¯å›ºå®š
            # åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥ï¼šé€£ç•ªé©ç”¨å¾Œã®æœ€çµ‚å½¢ã¯å›ºå®šæ‰±ã„
            "1003_å—ä¿¡é€šçŸ¥", "1013_å—ä¿¡é€šçŸ¥", "1023_å—ä¿¡é€šçŸ¥",  # éƒ½é“åºœçœŒå—ä¿¡é€šçŸ¥ï¼ˆå„ã‚»ãƒƒãƒˆï¼‰
            "2003_å—ä¿¡é€šçŸ¥", "2013_å—ä¿¡é€šçŸ¥", "2023_å—ä¿¡é€šçŸ¥"   # å¸‚ç”ºæ‘å—ä¿¡é€šçŸ¥ï¼ˆå„ã‚»ãƒƒãƒˆï¼‰
        }
        
        if template_id in FIXED_NUMBERS:
            print(f"[INFO] FIXED_NUMBERæ¤œå‡º: {template_id} -> æ­£è¦åŒ–ã‚¹ã‚­ãƒƒãƒ—")
            return 0, template_id, 0
        try:
            # 1. é€£ç•ªãƒãƒƒãƒ—ã‚’æ§‹ç¯‰
            pref_order_map, city_order_map = self.build_order_maps(set_settings)
            
            # 2. æ–‡æ›¸ç¨®åˆ¥ã‚’åˆ¤å®šï¼ˆéƒ½é“åºœçœŒç¨ vs å¸‚æ°‘ç¨ï¼‰
            doc_kind = "pref" if self._is_prefecture_tax_document(template_id) else "city"
            print(f"[INFO] æ–‡æ›¸ç¨®åˆ¥åˆ¤å®š: {doc_kind}")
            
            # 3. ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è‡ªæ²»ä½“ã‚»ãƒƒãƒˆIDã‚’è§£æ±º
            set_id = self.resolve_set_id_from_text(text, filename, set_settings, doc_kind)
            if not set_id:
                print(f"[WARN] è‡ªæ²»ä½“ã‚»ãƒƒãƒˆè§£æ±ºå¤±æ•—, ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæœ€åˆã®è©²å½“ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨
                for sid, info in set_settings.items():
                    if doc_kind == "pref":
                        set_id = sid
                        break
                    elif doc_kind == "city" and info.get("city", "").strip():
                        set_id = sid
                        break
                        
            if not set_id:
                print(f"[ERROR] ã‚»ãƒƒãƒˆIDè§£æ±ºå¤±æ•—")
                return 0, template_id, 0
            
            # 4. é€£ç•ªã‚³ãƒ¼ãƒ‰ã‚’æ±ºå®š
            if doc_kind == "pref":
                code = pref_order_map.get(set_id, 1001)
                pref = set_settings[set_id]["prefecture"]
                city = ""
                final_label = f"{code}_{pref}_éƒ½é“åºœçœŒç”³å‘Šæ›¸"
            else:
                code = city_order_map.get(set_id, 2001)
                pref = set_settings[set_id]["prefecture"]
                city = set_settings[set_id]["city"]
                final_label = f"{code}_{pref}{city}_å¸‚ç”ºæ‘ç”³å‘Šæ›¸"
            
            print(f"[INFO] æ­£è¦åŒ–çµæœ: set_id={set_id}, code={code}, pref={pref}, city={city}")
            print(f"[INFO] æœ€çµ‚ãƒ©ãƒ™ãƒ«: {final_label}")
            
            # 5. ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆIDãŒæœ€çµ‚å‡ºåŠ›ã«æ®‹ã‚‰ãªã„ã“ã¨ã‚’ç¢ºèª
            assert "å¸‚ç”ºæ‘" not in final_label, f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ–‡å­—åˆ—ãŒæ®‹å­˜: {final_label}"
            assert template_id != final_label, f"æ­£è¦åŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“: {template_id} == {final_label}"
            
            # 6. ä¸æ•´åˆæ¤œè¨¼ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            extracted_pref, extracted_city = self._extract_pref_city_from_text(text, filename)
            if extracted_pref and extracted_pref != pref:
                print(f"[WARN] Locality mismatch text=(pref={extracted_pref}, city={extracted_city}) vs set=(pref={pref}, city={city}), file={filename}")
            
            return code, final_label, set_id
            
        except Exception as e:
            print(f"[ERROR] æ­£è¦åŒ–å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            return 0, template_id, 0

    def _extract_municipality_info_from_text(self, text: str, filename: str) -> Tuple[Optional[int], Optional[int]]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è‡ªæ²»ä½“ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆUIè¨­å®šãƒ™ãƒ¼ã‚¹è§£æï¼‰"""
        combined_text = f"{text} {filename}".lower()
        
        # UIè¨­å®šãƒ™ãƒ¼ã‚¹ã®è‡ªæ²»ä½“åˆ¤å®šï¼ˆã‚ˆã‚ŠæŸ”è»Ÿãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ï¼‰
        # Step 1: ãƒ•ã‚¡ã‚¤ãƒ«åã¨å†…å®¹ã‹ã‚‰è‡ªæ²»ä½“ã‚’ç‰¹å®š
        submission_office_patterns = {
            1: ["æ±äº¬éƒ½", "æ¸¯éƒ½ç¨äº‹å‹™æ‰€", "èŠç¨å‹™ç½²", "éƒ½ç¨äº‹å‹™æ‰€", "æ±äº¬éƒ½æ¸¯éƒ½ç¨äº‹å‹™æ‰€"],  # ã‚»ãƒƒãƒˆ1
            2: ["æ„›çŸ¥çœŒ", "æ±ä¸‰æ²³çœŒç¨äº‹å‹™æ‰€", "è’²éƒ¡å¸‚", "è’²éƒ¡å¸‚å½¹æ‰€", "æ„›çŸ¥çœŒæ±ä¸‰æ²³çœŒç¨äº‹å‹™æ‰€"],  # ã‚»ãƒƒãƒˆ2  
            3: ["ç¦å²¡çœŒ", "è¥¿ç¦å²¡çœŒç¨äº‹å‹™æ‰€", "ç¦å²¡å¸‚", "ç¦å²¡å¸‚å½¹æ‰€", "ç¦å²¡çœŒè¥¿ç¦å²¡çœŒç¨äº‹å‹™æ‰€"],  # ã‚»ãƒƒãƒˆ3
            4: ["åŒ—æµ·é“", "æœ­å¹Œå¸‚", "é“ç¨äº‹å‹™æ‰€"],  # ã‚»ãƒƒãƒˆ4ï¼ˆæ‹¡å¼µç”¨ï¼‰
            5: ["å¤§é˜ªåºœ", "å¤§é˜ªå¸‚", "åºœç¨äº‹å‹™æ‰€"]   # ã‚»ãƒƒãƒˆ5ï¼ˆæ‹¡å¼µç”¨ï¼‰
        }
        
        # Step 2: ä¼šç¤¾ä½æ‰€ã‚’é™¤å¤–ã—ã¦ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆåˆ¤å®š
        # ä¼šç¤¾ä½æ‰€ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¿®æ­£æŒ‡ç¤ºæ›¸ã«åŸºã¥ãï¼‰
        company_address_patterns = [
            r'æ±äº¬éƒ½æ¸¯åŒºæ¸¯å—.*å“å·ã‚°ãƒ©ãƒ³ãƒ‰ã‚»ãƒ³ãƒˆãƒ©ãƒ«ã‚¿ãƒ¯ãƒ¼',
            r'æ„›çŸ¥çœŒè’²éƒ¡å¸‚è±Šå²¡ç”º.*44ç•ªåœ°',  
            r'ç¦å²¡çœŒç¦å²¡å¸‚ä¸­å¤®åŒºè‰é¦™æ±Ÿ'
        ]
        
        # ä¼šç¤¾ä½æ‰€ã‚’é™¤å¤–
        filtered_text = combined_text
        for pattern in company_address_patterns:
            filtered_text = re.sub(pattern, '', filtered_text, flags=re.IGNORECASE)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æå‡ºå…ˆã‚»ãƒƒãƒˆã‚’åˆ¤å®š
        detected_set = None
        for set_num, office_names in submission_office_patterns.items():
            for office_name in office_names:
                if office_name in filename.lower():
                    detected_set = set_num
                    self._log_debug(f"ãƒ•ã‚¡ã‚¤ãƒ«åæå‡ºå…ˆæ¤œå‡º: {office_name} â†’ ã‚»ãƒƒãƒˆ{set_num}")
                    break
            if detected_set:
                break
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚‚æå‡ºå…ˆã‚’ç¢ºèªï¼ˆä¼šç¤¾ä½æ‰€é™¤å¤–æ¸ˆã¿ï¼‰
        if not detected_set:
            for set_num, office_names in submission_office_patterns.items():
                for office_name in office_names:
                    if office_name in filtered_text:
                        detected_set = set_num
                        self._log_debug(f"ãƒ†ã‚­ã‚¹ãƒˆæå‡ºå…ˆæ¤œå‡º: {office_name} â†’ ã‚»ãƒƒãƒˆ{set_num}")
                        break
                if detected_set:
                    break
        
        # ã‚»ãƒƒãƒˆç•ªå·ã‚’æ­£ç¢ºãªé€£ç•ªã‚³ãƒ¼ãƒ‰ç•ªå·ã«å¤‰æ›ï¼ˆé€£ç•ªãƒ«ãƒ¼ãƒ«é©ç”¨ï¼‰
        prefecture_code = None
        municipality_code = None
        
        if detected_set:
            # éƒ½é“åºœçœŒç”³å‘Šæ›¸ã®é€£ç•ª: 1001 + (ã‚»ãƒƒãƒˆç•ªå·-1) Ã— 10
            prefecture_code = 1001 + (detected_set - 1) * 10
            self._log_debug(f"éƒ½é“åºœçœŒé€£ç•ªè¨ˆç®—: ã‚»ãƒƒãƒˆ{detected_set} â†’ 1001 + ({detected_set}-1)Ã—10 = {prefecture_code}")
            
            # å¸‚ç”ºæ‘ç”³å‘Šæ›¸ã®é€£ç•ª: 2001 + (ã‚»ãƒƒãƒˆç•ªå·-1) Ã— 10
            # ãŸã ã—ã€æ±äº¬éƒ½ï¼ˆã‚»ãƒƒãƒˆ1ï¼‰ã¯å¸‚ç”ºæ‘æ›¸é¡ãŒå­˜åœ¨ã—ãªã„ãŸã‚é™¤å¤–
            if detected_set > 1:  # æ±äº¬éƒ½ä»¥å¤–ã®å ´åˆã®ã¿
                municipality_code = 2001 + (detected_set - 1) * 10
                self._log_debug(f"å¸‚ç”ºæ‘é€£ç•ªè¨ˆç®—: ã‚»ãƒƒãƒˆ{detected_set} â†’ 2001 + ({detected_set}-1)Ã—10 = {municipality_code}")
            else:
                self._log_debug(f"æ±äº¬éƒ½ï¼ˆã‚»ãƒƒãƒˆ1ï¼‰ã¯å¸‚ç”ºæ‘æ›¸é¡ãªã—")
        
        self._log_debug(f"ãƒ†ã‚­ã‚¹ãƒˆè‡ªæ²»ä½“èªè­˜çµæœ: éƒ½é“åºœçœŒ={prefecture_code}, å¸‚ç”ºæ‘={municipality_code}")
        return prefecture_code, municipality_code
    
    def _is_payment_info(self, text_content: str, filename: str) -> bool:
        """
        ä¿®æ­£æŒ‡ç¤ºæ›¸: ä¿®æ­£3 - ç´ä»˜æƒ…å ±ã®åˆ¤å®šå¼·åŒ–
        ç´ä»˜æƒ…å ±ã¨ã—ã¦å¿…ãšåˆ†é¡ã™ã¹ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
        """
        payment_indicators = [
            'ç´ä»˜åŒºåˆ†ç•ªå·é€šçŸ¥',
            'ç´ä»˜å†…å®¹ã‚’ç¢ºèªã—',
            'ä»¥ä¸‹ã®ãƒœã‚¿ãƒ³ã‚ˆã‚Šç´ä»˜',
            'ãƒ¡ãƒ¼ãƒ«è©³ç´°ï¼ˆç´ä»˜åŒºåˆ†ç•ªå·é€šçŸ¥ï¼‰'
        ]
        
        # ã“ã‚Œã‚‰ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹å ´åˆã¯å¿…ãšç´ä»˜æƒ…å ±ã¨ã—ã¦åˆ†é¡
        for indicator in payment_indicators:
            if indicator in text_content:
                self._log_debug(f"ç´ä»˜æƒ…å ±å¼·åˆ¶åˆ¤å®š: {indicator}")
                return True
        
        return False
    
    def _is_receipt_notification(self, text_content: str, filename: str) -> bool:
        """
        ä¿®æ­£æŒ‡ç¤ºæ›¸: ä¿®æ­£3 - å—ä¿¡é€šçŸ¥ã®åˆ¤å®šå¼·åŒ–
        ç´ä»˜é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒãªã„å ´åˆã®ã¿å—ä¿¡é€šçŸ¥ã¨ã™ã‚‹
        """
        receipt_indicators = [
            'é€ä¿¡ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’å—ã‘ä»˜ã‘ã¾ã—ãŸ',
            'ç”³å‘Šãƒ‡ãƒ¼ã‚¿ã‚’å—ä»˜ã‘ã¾ã—ãŸ',
            'ãƒ¡ãƒ¼ãƒ«è©³ç´°'  # å˜ç‹¬ã®å ´åˆ
        ]
        
        # ç´ä»˜é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆã¯å—ä¿¡é€šçŸ¥ã‹ã‚‰é™¤å¤–
        exclusion_keywords = ['ç´ä»˜åŒºåˆ†ç•ªå·é€šçŸ¥', 'ç´ä»˜å†…å®¹ã‚’ç¢ºèªã—', 'ãƒ¡ãƒ¼ãƒ«è©³ç´°ï¼ˆç´ä»˜åŒºåˆ†ç•ªå·é€šçŸ¥ï¼‰']
        
        has_receipt = any(indicator in text_content for indicator in receipt_indicators)
        has_payment = any(keyword in text_content for keyword in exclusion_keywords)
        
        result = has_receipt and not has_payment
        if result:
            self._log_debug(f"å—ä¿¡é€šçŸ¥å¼·åˆ¶åˆ¤å®š: å—ä¿¡={has_receipt}, ç´ä»˜é™¤å¤–={has_payment}")
        
        return result
    
    def _classify_local_tax_receipt(self, text: str, filename: str, prefecture_code: Optional[int], municipality_code: Optional[int]) -> Optional[ClassificationResult]:
        """
        åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥ã®å°‚ç”¨åˆ†é¡å‡¦ç†ï¼ˆOCRãƒ™ãƒ¼ã‚¹è‡ªæ²»ä½“ã‚»ãƒƒãƒˆç…§åˆå¯¾å¿œç‰ˆï¼‰
        éƒ½é“åºœçœŒãƒ»å¸‚ç”ºæ‘ã®å—ä¿¡é€šçŸ¥ã«é©åˆ‡ãªé€£ç•ªã‚’ä»˜ä¸
        """
        combined_text = f"{text} {filename}"
        
        # éƒ½é“åºœçœŒå‘ã‘å—ä¿¡é€šçŸ¥ã®åˆ¤å®šæ¡ä»¶ï¼ˆANDæ¡ä»¶ï¼‰
        prefecture_receipt_conditions = [
            ["ç”³å‘Šå—ä»˜å®Œäº†é€šçŸ¥", "æ³•äººäº‹æ¥­ç¨"],
            ["ç”³å‘Šå—ä»˜å®Œäº†é€šçŸ¥", "ç‰¹åˆ¥æ³•äººäº‹æ¥­ç¨"], 
            ["çœŒç¨äº‹å‹™æ‰€", "å—ä¿¡é€šçŸ¥"],
            ["éƒ½ç¨äº‹å‹™æ‰€", "å—ä¿¡é€šçŸ¥"]
        ]
        
        # å¸‚ç”ºæ‘å‘ã‘å—ä¿¡é€šçŸ¥ã®åˆ¤å®šæ¡ä»¶ï¼ˆANDæ¡ä»¶ï¼‰
        municipality_receipt_conditions = [
            ["ç”³å‘Šå—ä»˜å®Œäº†é€šçŸ¥", "å¸‚ç”ºæ‘ç”³å‘Šæ›¸"],
            ["ç”³å‘Šå—ä»˜å®Œäº†é€šçŸ¥", "æ³•äººå¸‚ç”ºæ‘æ°‘ç¨"],
            ["å¸‚å½¹æ‰€", "ç”³å‘Šå—ä»˜å®Œäº†é€šçŸ¥"],
            ["å¸‚é•·", "å¸‚ç”ºæ‘ç”³å‘Šæ›¸", "å—ä»˜å®Œäº†é€šçŸ¥"],
            ["å¸‚å½¹æ‰€", "å¸‚ç”ºæ‘ç”³å‘Šæ›¸ç”³å‘Šæ›¸", "å—ä»˜å®Œäº†é€šçŸ¥"],  # è¿½åŠ 
            ["å¸‚ç”ºæ‘ç”³å‘Šæ›¸ç”³å‘Šæ›¸", "å—ä»˜å®Œäº†é€šçŸ¥"]  # è¿½åŠ 
        ]
        
        # éƒ½é“åºœçœŒå‘ã‘åˆ¤å®š
        if any(all(kw in combined_text for kw in condition) 
               for condition in prefecture_receipt_conditions):
            # OCRãƒ™ãƒ¼ã‚¹è‡ªæ²»ä½“ã‚»ãƒƒãƒˆç…§åˆã‚’ä½¿ç”¨
            set_number = self._detect_municipality_set_from_text(text, filename, "prefecture")
            if not set_number:
                set_number = 1  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            code = self._generate_receipt_number("prefecture", set_number)
            self._log_debug(f"åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥ï¼ˆéƒ½é“åºœçœŒï¼‰: OCRã‚»ãƒƒãƒˆæ¤œå‡º={set_number} â†’ {code}_å—ä¿¡é€šçŸ¥")
            return ClassificationResult(
                document_type=f"{code}_å—ä¿¡é€šçŸ¥",
                confidence=1.0,
                matched_keywords=["åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥ï¼ˆéƒ½é“åºœçœŒï¼‰"],
                classification_method="local_tax_receipt_detection_ocr",
                debug_steps=[],
                processing_log=self.processing_log.copy()
            )
        
        # å¸‚ç”ºæ‘å‘ã‘åˆ¤å®š  
        if any(all(kw in combined_text for kw in condition)
               for condition in municipality_receipt_conditions):
            # OCRãƒ™ãƒ¼ã‚¹è‡ªæ²»ä½“ã‚»ãƒƒãƒˆç…§åˆã‚’ä½¿ç”¨ï¼ˆæ±äº¬ç¹°ã‚Šä¸ŠãŒã‚Šå¯¾å¿œï¼‰
            set_number = self._detect_municipality_set_from_text(text, filename, "municipality")
            if not set_number:
                set_number = 2  # å¸‚ç”ºæ‘ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆæ±äº¬ãŒã‚ã‚‹å ´åˆã¯2ã‹ã‚‰é–‹å§‹ï¼‰
            code = self._generate_receipt_number("municipality", set_number)
            self._log_debug(f"åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥ï¼ˆå¸‚ç”ºæ‘ï¼‰: OCRã‚»ãƒƒãƒˆæ¤œå‡º={set_number} â†’ {code}_å—ä¿¡é€šçŸ¥")
            return ClassificationResult(
                document_type=f"{code}_å—ä¿¡é€šçŸ¥",
                confidence=1.0,
                matched_keywords=["åœ°æ–¹ç¨å—ä¿¡é€šçŸ¥ï¼ˆå¸‚ç”ºæ‘ï¼‰"],
                classification_method="local_tax_receipt_detection_ocr",
                debug_steps=[],
                processing_log=self.processing_log.copy()
            )
            
        return None
    
    def _generate_receipt_number(self, classification_type: str, jurisdiction_set_number: int) -> str:
        """
        å—ä¿¡é€šçŸ¥ã®é€£ç•ªã‚’ç”Ÿæˆï¼ˆæ±äº¬éƒ½ç¹°ã‚Šä¸ŠãŒã‚Šå¯¾å¿œç‰ˆï¼‰
        
        Args:
            classification_type: "prefecture" or "municipality"
            jurisdiction_set_number: ã‚»ãƒƒãƒˆç•ªå· (1-5)
        
        Returns:
            str: ç”Ÿæˆã•ã‚ŒãŸç•ªå· (ä¾‹: "1003", "2013")
        """
        base_numbers = {
            "prefecture": 1003,
            "municipality": 2003
        }
        
        if classification_type in base_numbers:
            base = base_numbers[classification_type]
            
            # æ±äº¬éƒ½ãŒè¨­å®šã«ã‚ã‚‹å ´åˆã®ç¹°ã‚Šä¸ŠãŒã‚Šãƒ­ã‚¸ãƒƒã‚¯ï¼ˆç”³å‘Šæ›¸ã¨åŒæ§˜ï¼‰
            # ã‚»ãƒƒãƒˆè¨­å®šã‹ã‚‰æ±äº¬éƒ½ã®å­˜åœ¨ã‚’ç¢ºèª
            tokyo_offset = 0
            if hasattr(self, 'current_municipality_sets') and self.current_municipality_sets:
                for set_id, set_info in self.current_municipality_sets.items():
                    if set_info.get("prefecture") == "æ±äº¬éƒ½":
                        tokyo_offset = 1
                        break
                        
            # å¸‚ç”ºæ‘ã®å ´åˆã€æ±äº¬éƒ½ãŒã‚ã‚‹ã¨ãƒ™ãƒ¼ã‚¹ãŒç¹°ã‚Šä¸ŠãŒã‚‹
            if classification_type == "municipality" and tokyo_offset > 0:
                # æ±äº¬éƒ½ãŒã‚»ãƒƒãƒˆ1ã«ã‚ã‚‹å ´åˆã€å¸‚ç”ºæ‘å—ä¿¡é€šçŸ¥ã¯2003ã‹ã‚‰é–‹å§‹
                # ã‚»ãƒƒãƒˆ2â†’2013, ã‚»ãƒƒãƒˆ3â†’2023, ã‚»ãƒƒãƒˆ4â†’2033...
                result = str(base + (jurisdiction_set_number - 1) * 10)
            else:
                result = str(base + (jurisdiction_set_number - 1) * 10)
                
            self._log_debug(f"é€£ç•ªç”Ÿæˆ: {classification_type} ã‚»ãƒƒãƒˆ{jurisdiction_set_number} tokyo_offset={tokyo_offset} â†’ {result}")
            return result
        
        return "0003"  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    
    def _detect_municipality_set_from_text(self, text: str, filename: str, target_type: str) -> Optional[int]:
        """
        OCRãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è‡ªæ²»ä½“ã‚»ãƒƒãƒˆç•ªå·ã‚’æ¤œå‡ºï¼ˆç”³å‘Šæ›¸ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
        
        Args:
            text: OCRãƒ†ã‚­ã‚¹ãƒˆ
            filename: ãƒ•ã‚¡ã‚¤ãƒ«å 
            target_type: "prefecture" or "municipality"
            
        Returns:
            int: ã‚»ãƒƒãƒˆç•ªå·ã€æ¤œå‡ºã§ããªã„å ´åˆã¯None
        """
        combined_text = f"{text} {filename}"
        
        # è‡ªæ²»ä½“ã‚»ãƒƒãƒˆè¨­å®šã‚’å–å¾—
        if not hasattr(self, 'current_municipality_sets') or not self.current_municipality_sets:
            self._log_debug(f"OCRè‡ªæ²»ä½“ã‚»ãƒƒãƒˆæ¤œå‡º: ã‚»ãƒƒãƒˆè¨­å®šãªã—")
            return None
            
        # å„ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒãƒãƒ³ã‚°
        for set_id, set_info in self.current_municipality_sets.items():
            prefecture = set_info.get("prefecture", "")
            city = set_info.get("city", "")
            
            # éƒ½é“åºœçœŒå—ä¿¡é€šçŸ¥ã®å ´åˆã¯éƒ½é“åºœçœŒåã§ãƒãƒƒãƒãƒ³ã‚°
            if target_type == "prefecture":
                if prefecture and prefecture in combined_text:
                    self._log_debug(f"OCRè‡ªæ²»ä½“ã‚»ãƒƒãƒˆæ¤œå‡º: {prefecture}ãŒãƒ†ã‚­ã‚¹ãƒˆã§æ¤œå‡º â†’ ã‚»ãƒƒãƒˆ{set_id}")
                    return set_id
                    
            # å¸‚ç”ºæ‘å—ä¿¡é€šçŸ¥ã®å ´åˆã¯å¸‚ç”ºæ‘åã§ãƒãƒƒãƒãƒ³ã‚°
            elif target_type == "municipality":
                if city and city in combined_text:
                    self._log_debug(f"OCRè‡ªæ²»ä½“ã‚»ãƒƒãƒˆæ¤œå‡º: {city}ãŒãƒ†ã‚­ã‚¹ãƒˆã§æ¤œå‡º â†’ ã‚»ãƒƒãƒˆ{set_id}")
                    return set_id
                # å¸‚ç”ºæ‘åãŒãªã„å ´åˆï¼ˆæ±äº¬éƒ½ç­‰ï¼‰ã¯éƒ½é“åºœçœŒåã§ãƒãƒƒãƒãƒ³ã‚°
                elif not city and prefecture and prefecture in combined_text:
                    # æ±äº¬éƒ½ã®å ´åˆã€å¸‚ç”ºæ‘å—ä¿¡é€šçŸ¥ã§ã‚‚æ±äº¬éƒ½ãƒãƒƒãƒã§è©²å½“ã‚»ãƒƒãƒˆã‚’è¿”ã™
                    # ãŸã ã—ã€æ±äº¬éƒ½ã¯åŸºæœ¬çš„ã«å¸‚ç”ºæ‘ç¨ãŒãªã„ãŸã‚ã€ç¨€ãªã‚±ãƒ¼ã‚¹
                    self._log_debug(f"OCRè‡ªæ²»ä½“ã‚»ãƒƒãƒˆæ¤œå‡º: {prefecture}ï¼ˆå¸‚ç”ºæ‘ãªã—ï¼‰ãŒãƒ†ã‚­ã‚¹ãƒˆã§æ¤œå‡º â†’ ã‚»ãƒƒãƒˆ{set_id}")
                    return set_id
                    
        self._log_debug(f"OCRè‡ªæ²»ä½“ã‚»ãƒƒãƒˆæ¤œå‡º: ãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒãƒãƒ³ã‚°å¤±æ•—")
        return None
    
    def _get_jurisdiction_set_number(self, code: Optional[int], jurisdiction_type: str) -> int:
        """
        ãƒã‚°ä¿®æ­£ä¾é ¼æ›¸: A-2 è‡ªæ²»ä½“ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã‚»ãƒƒãƒˆç•ªå·ã‚’å–å¾—
        
        Args:
            code: éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ã¾ãŸã¯å¸‚ç”ºæ‘ã‚³ãƒ¼ãƒ‰
            jurisdiction_type: "prefecture" or "municipality"
        
        Returns:
            int: ã‚»ãƒƒãƒˆç•ªå· (1-5)
        """
        if jurisdiction_type == "prefecture":
            # éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã‚»ãƒƒãƒˆç•ªå·ã¸ã®å¤‰æ›
            code_to_set = {
                1001: 1,  # æ±äº¬éƒ½
                1011: 2,  # æ„›çŸ¥çœŒ 
                1021: 3,  # ç¦å²¡çœŒ
                1031: 4,  # å°†æ¥ã®æ‹¡å¼µç”¨
                1041: 5   # å°†æ¥ã®æ‹¡å¼µç”¨
            }
            return code_to_set.get(code, 1)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚»ãƒƒãƒˆ1
        elif jurisdiction_type == "municipality":
            # å¸‚ç”ºæ‘ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã‚»ãƒƒãƒˆç•ªå·ã¸ã®å¤‰æ›
            code_to_set = {
                2001: 1,  # 1ç•ªç›®ã®å¸‚ç”ºæ‘
                2011: 2,  # 2ç•ªç›®ã®å¸‚ç”ºæ‘
                2021: 3,  # 3ç•ªç›®ã®å¸‚ç”ºæ‘
                2031: 4,  # 4ç•ªç›®ã®å¸‚ç”ºæ‘
                2041: 5   # 5ç•ªç›®ã®å¸‚ç”ºæ‘
            }
            return code_to_set.get(code, 1)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚»ãƒƒãƒˆ1
        
        return 1  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    
    def _check_enhanced_payment_receipt_detection(self, text: str, filename: str) -> Optional[ClassificationResult]:
        """
        ä¿®æ­£æŒ‡ç¤ºæ›¸: ä¿®æ­£3 - ç´ä»˜æƒ…å ±ãƒ»å—ä¿¡é€šçŸ¥ã®åˆ¤åˆ¥å¼·åŒ–
        ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãå¼·åˆ¶åˆ†é¡
        """
        combined_text = f"{text} {filename}"
        
        # ç´ä»˜æƒ…å ±ã®å¼·åˆ¶åˆ¤å®š
        if self._is_payment_info(combined_text, filename):
            # ç¨ç›®ã«ã‚ˆã£ã¦é©åˆ‡ãªç´ä»˜æƒ…å ±ã‚³ãƒ¼ãƒ‰ã‚’è¿”ã™
            if "æ³•äººç¨" in combined_text or "å†…å›½æ³•äºº" in combined_text:
                return ClassificationResult(
                    document_type="0004_ç´ä»˜æƒ…å ±",
                    confidence=1.0,
                    matched_keywords=["ç´ä»˜æƒ…å ±å¼·åˆ¶åˆ¤å®š"],
                    classification_method="enhanced_payment_detection",
                    debug_steps=[],
                    processing_log=self.processing_log.copy()
                )
            elif "æ¶ˆè²»ç¨" in combined_text:
                return ClassificationResult(
                    document_type="3004_ç´ä»˜æƒ…å ±",
                    confidence=1.0,
                    matched_keywords=["ç´ä»˜æƒ…å ±å¼·åˆ¶åˆ¤å®š"],
                    classification_method="enhanced_payment_detection",
                    debug_steps=[],
                    processing_log=self.processing_log.copy()
                )
            elif "éƒ½é“åºœçœŒ" in combined_text or "çœŒç¨äº‹å‹™æ‰€" in combined_text or "éƒ½ç¨äº‹å‹™æ‰€" in combined_text:
                return ClassificationResult(
                    document_type="1004_ç´ä»˜æƒ…å ±",
                    confidence=1.0,
                    matched_keywords=["ç´ä»˜æƒ…å ±å¼·åˆ¶åˆ¤å®š"],
                    classification_method="enhanced_payment_detection",
                    debug_steps=[],
                    processing_log=self.processing_log.copy()
                )
            elif "å¸‚ç”ºæ‘" in combined_text or "å¸‚å½¹æ‰€" in combined_text or "å¸‚æ°‘ç¨" in combined_text:
                return ClassificationResult(
                    document_type="2004_ç´ä»˜æƒ…å ±",
                    confidence=1.0,
                    matched_keywords=["ç´ä»˜æƒ…å ±å¼·åˆ¶åˆ¤å®š"],
                    classification_method="enhanced_payment_detection",
                    debug_steps=[],
                    processing_log=self.processing_log.copy()
                )
        
        # å—ä¿¡é€šçŸ¥ã®å¼·åˆ¶åˆ¤å®šï¼ˆç´ä»˜é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒãªã„å ´åˆã®ã¿ï¼‰
        if self._is_receipt_notification(combined_text, filename):
            # ç¨ç›®ã«ã‚ˆã£ã¦é©åˆ‡ãªå—ä¿¡é€šçŸ¥ã‚³ãƒ¼ãƒ‰ã‚’è¿”ã™
            if "æ³•äººç¨" in combined_text or "å†…å›½æ³•äºº" in combined_text:
                return ClassificationResult(
                    document_type="0003_å—ä¿¡é€šçŸ¥",
                    confidence=1.0,
                    matched_keywords=["å—ä¿¡é€šçŸ¥å¼·åˆ¶åˆ¤å®š"],
                    classification_method="enhanced_receipt_detection",
                    debug_steps=[],
                    processing_log=self.processing_log.copy()
                )
            elif "æ¶ˆè²»ç¨" in combined_text:
                return ClassificationResult(
                    document_type="3003_å—ä¿¡é€šçŸ¥",
                    confidence=1.0,
                    matched_keywords=["å—ä¿¡é€šçŸ¥å¼·åˆ¶åˆ¤å®š"],
                    classification_method="enhanced_receipt_detection",
                    debug_steps=[],
                    processing_log=self.processing_log.copy()
                )
            elif "éƒ½é“åºœçœŒ" in combined_text or "çœŒç¨äº‹å‹™æ‰€" in combined_text or "éƒ½ç¨äº‹å‹™æ‰€" in combined_text:
                return ClassificationResult(
                    document_type="1003_å—ä¿¡é€šçŸ¥",
                    confidence=1.0,
                    matched_keywords=["å—ä¿¡é€šçŸ¥å¼·åˆ¶åˆ¤å®š"],
                    classification_method="enhanced_receipt_detection",
                    debug_steps=[],
                    processing_log=self.processing_log.copy()
                )
            elif "å¸‚ç”ºæ‘" in combined_text or "å¸‚å½¹æ‰€" in combined_text or "å¸‚æ°‘ç¨" in combined_text:
                return ClassificationResult(
                    document_type="2003_å—ä¿¡é€šçŸ¥",
                    confidence=1.0,
                    matched_keywords=["å—ä¿¡é€šçŸ¥å¼·åˆ¶åˆ¤å®š"],
                    classification_method="enhanced_receipt_detection",
                    debug_steps=[],
                    processing_log=self.processing_log.copy()
                )
        
        return None
    
    # ===== v5.2 Bundle PDF Support Methods =====
    
    def detect_page_doc_code(self, text: str, prefer_bundle: Optional[str] = None) -> Optional[str]:
        """
        ãƒšãƒ¼ã‚¸å†…ã‚³ãƒ¼ãƒ‰æ¨å®š (v5.2 Bundle PDF Support)
        å˜ä¸€ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ›¸é¡ã‚³ãƒ¼ãƒ‰ã‚’æ¨å®š
        
        Args:
            text: ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹
            prefer_bundle: ãƒãƒ³ãƒ‰ãƒ«ç¨®åˆ¥ã®ãƒ’ãƒ³ãƒˆ ("local", "national", None)
            
        Returns:
            str|None: æ¨å®šã•ã‚ŒãŸæ›¸é¡ã‚³ãƒ¼ãƒ‰ (ä¾‹: "0003", "1003") ã¾ãŸã¯None
        """
        if not text:
            return None
        
        # å¼·åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå„ªå…ˆï¼‰: ç›´æ¥çš„ãªã‚³ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
        strong_pattern = re.compile(r'\b(0003|0004|3003|3004|1003|1013|1023|1004|2003|2013|2023|2004)\b')
        strong_match = strong_pattern.search(text)
        if strong_match:
            code = strong_match.group(1)
            self._log_debug(f"Strong code pattern detected: {code}")
            return code
        
        # è£œåŠ©ãƒ‘ã‚¿ãƒ¼ãƒ³: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰çµ„ã¿åˆã‚ã›åˆ¤å®š
        is_receipt = any(kw in text for kw in ["å—ä¿¡é€šçŸ¥", "ç”³å‘Šå—ä»˜å®Œäº†é€šçŸ¥", "ç”³å‘Šå—ä»˜å®Œäº†", "å—ä»˜å®Œäº†é€šçŸ¥"])
        is_payment = any(kw in text for kw in ["ç´ä»˜æƒ…å ±", "ç´ä»˜åŒºåˆ†ç•ªå·é€šçŸ¥", "ç´ä»˜æ›¸", "ç´ä»˜æƒ…å ±ç™ºè¡Œçµæœ"])
        
        # ç¨ç›®åˆ¥ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        has_corporation_tax = any(kw in text for kw in ["æ³•äººç¨", "å†…å›½æ³•äºº", "æ³•äººç¨åŠã³åœ°æ–¹æ³•äººç¨"])
        has_consumption_tax = any(kw in text for kw in ["æ¶ˆè²»ç¨", "åœ°æ–¹æ¶ˆè²»ç¨", "æ¶ˆè²»ç¨åŠã³åœ°æ–¹æ¶ˆè²»ç¨"])
        has_prefecture = any(kw in text for kw in ["éƒ½é“åºœçœŒ", "çœŒç¨äº‹å‹™æ‰€", "éƒ½ç¨äº‹å‹™æ‰€", "æ³•äººäº‹æ¥­ç¨", "ç‰¹åˆ¥æ³•äººäº‹æ¥­ç¨"])
        has_municipality = any(kw in text for kw in ["å¸‚ç”ºæ‘", "å¸‚å½¹æ‰€", "å¸‚ç”ºæ‘ç”³å‘Šæ›¸", "å¸‚ç”ºæ‘æ°‘ç¨"])
        
        # è‡ªæ²»ä½“ç‰¹å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        has_specific_local = any(kw in text for kw in ["æ±äº¬éƒ½", "æ„›çŸ¥çœŒ", "ç¦å²¡çœŒ", "è’²éƒ¡å¸‚", "ç¦å²¡å¸‚"])
        
        # å›½ç¨ç³»ã®åˆ¤å®š
        if is_receipt and has_corporation_tax:
            self._log_debug("Receipt + Corporation tax detected -> 0003")
            return "0003"
        elif is_payment and has_corporation_tax:
            self._log_debug("Payment + Corporation tax detected -> 0004")  
            return "0004"
        elif is_receipt and has_consumption_tax:
            self._log_debug("Receipt + Consumption tax detected -> 3003")
            return "3003"
        elif is_payment and has_consumption_tax:
            self._log_debug("Payment + Consumption tax detected -> 3004")
            return "3004"
        
        # åœ°æ–¹ç¨ç³»ã®åˆ¤å®š
        elif is_receipt and (has_prefecture or has_specific_local):
            # éƒ½é“åºœçœŒå—ä¿¡é€šçŸ¥ã¯é€£ç•ªå¯¾å¿œãŒå¿…è¦ãªãŸã‚åŸºæœ¬ã‚³ãƒ¼ãƒ‰ã‚’è¿”ã™
            self._log_debug("Receipt + Prefecture detected -> 1003 (base code)")
            return "1003"  # å¾Œç¶šã§é€£ç•ªè£œæ­£ã•ã‚Œã‚‹
        elif is_payment and (has_prefecture or has_specific_local):
            self._log_debug("Payment + Prefecture detected -> 1004")
            return "1004"
        elif is_receipt and has_municipality:
            # å¸‚ç”ºæ‘å—ä¿¡é€šçŸ¥ã‚‚é€£ç•ªå¯¾å¿œãŒå¿…è¦
            self._log_debug("Receipt + Municipality detected -> 2003 (base code)")
            return "2003"  # å¾Œç¶šã§é€£ç•ªè£œæ­£ã•ã‚Œã‚‹
        elif is_payment and has_municipality:
            self._log_debug("Payment + Municipality detected -> 2004")
            return "2004"
        
        # prefer_bundleã«åŸºã¥ããƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯åˆ¤å®š
        if prefer_bundle == "national":
            if is_receipt:
                self._log_debug(f"National bundle heuristic: receipt -> 0003 (prefer)")
                return "0003"  # æ³•äººç¨ã‚’å„ªå…ˆ
            elif is_payment:
                self._log_debug(f"National bundle heuristic: payment -> 0004 (prefer)")
                return "0004"  # æ³•äººç¨ã‚’å„ªå…ˆ
        elif prefer_bundle == "local":
            if is_receipt:
                self._log_debug(f"Local bundle heuristic: receipt -> 1003 (prefer)")
                return "1003"  # éƒ½é“åºœçœŒã‚’å„ªå…ˆ
            elif is_payment:
                # åœ°æ–¹ç¨ã®ç´ä»˜æƒ…å ±ã¯éƒ½é“åºœçœŒãƒ»å¸‚ç”ºæ‘ã§åˆ†ã‹ã‚Œã‚‹ãŸã‚ã€ã‚ˆã‚Šæ…é‡ã«åˆ¤å®š
                if has_prefecture or not has_municipality:
                    self._log_debug(f"Local bundle heuristic: payment -> 1004 (prefer prefecture)")
                    return "1004"
                else:
                    self._log_debug(f"Local bundle heuristic: payment -> 2004 (prefer municipality)")
                    return "2004"
        
        # åˆ¤å®šã§ããªã„å ´åˆ
        self._log_debug("No code pattern detected")
        return None

    def export_keyword_dictionary(self) -> str:
        """
        åˆ†é¡ãƒ«ãƒ¼ãƒ«è¾æ›¸ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹
        
        Returns:
            str: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ãƒ«ãƒ‘ã‚¹
        """
        # ç¾åœ¨æ—¥æ™‚ã§ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"keyword_dictionary_{timestamp}.json"
        
        # ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ãƒ‘ã‚¹ã‚’å–å¾—
        desktop_path = Path.home() / "Desktop"
        file_path = desktop_path / filename
        
        # è¾æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        export_data = {
            "export_info": {
                "timestamp": datetime.datetime.now().isoformat(),
                "version": "DocumentClassifierV5",
                "total_rules": len(self.classification_rules_v5),
                "description": "æ›¸é¡åˆ†é¡ã‚¨ãƒ³ã‚¸ãƒ³ v5.0 - åˆ†é¡ãƒ«ãƒ¼ãƒ«è¾æ›¸"
            },
            "classification_rules": {}
        }
        
        # å„åˆ†é¡ãƒ«ãƒ¼ãƒ«ã‚’æ•´ç†ã—ã¦ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        for doc_type, rules in self.classification_rules_v5.items():
            export_data["classification_rules"][doc_type] = {
                "priority": rules.get("priority", 0),
                "highest_priority_conditions": [],
                "exact_keywords": rules.get("exact_keywords", []),
                "partial_keywords": rules.get("partial_keywords", []),
                "exclude_keywords": rules.get("exclude_keywords", []),
                "filename_keywords": rules.get("filename_keywords", [])
            }
            
            # ANDæ¡ä»¶ã‚’æ–‡å­—åˆ—ãƒªã‚¹ãƒˆã«å¤‰æ›
            for condition in rules.get("highest_priority_conditions", []):
                condition_dict = {
                    "keywords": condition.keywords,
                    "match_type": condition.match_type
                }
                export_data["classification_rules"][doc_type]["highest_priority_conditions"].append(condition_dict)
        
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ï¼ˆèª­ã¿ã‚„ã™ã„å½¢å¼ã§ï¼‰
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2, sort_keys=True)
            
            # ãƒ­ã‚°å‡ºåŠ›
            self._log(f"åˆ†é¡ãƒ«ãƒ¼ãƒ«è¾æ›¸ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ: {file_path}")
            return str(file_path)
            
        except Exception as e:
            error_msg = f"ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            self._log(error_msg, "ERROR")
            raise Exception(error_msg)

    def export_keyword_dictionary_markdown(self) -> str:
        """
        åˆ†é¡ãƒ«ãƒ¼ãƒ«è¾æ›¸ã‚’å¤§å­¦ç”Ÿã«ã‚‚åˆ†ã‹ã‚Šã‚„ã™ã„Markdownãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹
        æ—¥æœ¬èªã¨çµµæ–‡å­—ã‚’ä½¿ã£ã¦ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªå½¢å¼ã§å‡ºåŠ›
        
        Returns:
            str: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ãƒ«ãƒ‘ã‚¹
        """
        # ç¾åœ¨æ—¥æ™‚ã§ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¾æ›¸_{timestamp}.md"
        
        # ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—ãƒ‘ã‚¹ã‚’å–å¾—
        desktop_path = Path.home() / "Desktop"
        file_path = desktop_path / filename
        
        # Markdownã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ
        try:
            markdown_content = self._generate_markdown_content()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            # ãƒ­ã‚°å‡ºåŠ›
            self._log(f"Markdownå½¢å¼ã®åˆ†é¡ãƒ«ãƒ¼ãƒ«è¾æ›¸ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ: {file_path}")
            return str(file_path)
            
        except Exception as e:
            error_msg = f"Markdownã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            self._log(error_msg, "ERROR")
            raise Exception(error_msg)
    
    def _generate_markdown_content(self) -> str:
        """Markdownã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆã™ã‚‹"""
        # æ›¸é¡åˆ†é¡ã®æ¦‚è¦
        content = []
        content.append("# ğŸ“‹ ç¨å‹™æ›¸é¡åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¾æ›¸\n")
        content.append(f"**ä½œæˆæ—¥æ™‚**: {datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\n")
        content.append(f"**ã‚·ã‚¹ãƒ†ãƒ ç‰ˆæœ¬**: DocumentClassifierV5\n")
        content.append(f"**åˆ†é¡ãƒ«ãƒ¼ãƒ«ç·æ•°**: {len(self.classification_rules_v5)}ä»¶\n\n")
        
        # ã‚·ã‚¹ãƒ†ãƒ ã®ä»•çµ„ã¿èª¬æ˜
        content.append("## ğŸ“ ã‚·ã‚¹ãƒ†ãƒ ã®ä»•çµ„ã¿ï¼ˆå¤§å­¦ç”Ÿå‘ã‘è§£èª¬ï¼‰\n")
        content.append("### ğŸ“š åŸºæœ¬çš„ãªå‹•ä½œåŸç†\n")
        content.append("ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯ã€PDFã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚„ãƒ•ã‚¡ã‚¤ãƒ«åã‚’åˆ†æã—ã¦ã€ã©ã®ç¨®é¡ã®ç¨å‹™æ›¸é¡ã‹ã‚’è‡ªå‹•åˆ¤å®šã—ã¾ã™ã€‚\n")
        content.append("åˆ¤å®šã¯ä»¥ä¸‹ã®3ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã§è¡Œã‚ã‚Œã¾ã™ï¼š\n\n")
        
        content.append("1. **ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢**: æ–‡æ›¸å†…ã§ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¢ã—ã¾ã™\n")
        content.append("2. **â­ ã‚¹ã‚³ã‚¢è¨ˆç®—**: è¦‹ã¤ã‹ã£ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é‡è¦åº¦ã«åŸºã¥ã„ã¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã¾ã™\n")
        content.append("3. **ğŸ† æœ€çµ‚åˆ¤å®š**: æœ€ã‚‚é«˜ã„ã‚¹ã‚³ã‚¢ã®æ›¸é¡ç¨®åˆ¥ã‚’é¸æŠã—ã¾ã™\n\n")
        
        content.append("### ğŸ“Š å„ªå…ˆåº¦ã‚·ã‚¹ãƒ†ãƒ \n")
        content.append("å„æ›¸é¡ã«ã¯å„ªå…ˆåº¦ï¼ˆPriorityï¼‰ãŒè¨­å®šã•ã‚Œã¦ãŠã‚Šã€æ•°å€¤ãŒå¤§ãã„ã»ã©å„ªå…ˆã•ã‚Œã¾ã™ï¼š\n")
        content.append("- **200**: æœ€é«˜å„ªå…ˆåº¦ï¼ˆç¢ºå®Ÿã«åˆ¤å®šã—ãŸã„é‡è¦æ›¸é¡ï¼‰\n")
        content.append("- **180**: é«˜å„ªå…ˆåº¦\n")
        content.append("- **150-140**: ä¸­å„ªå…ˆåº¦\n")
        content.append("- **135-130**: æ¨™æº–å„ªå…ˆåº¦\n\n")
        
        content.append("### ğŸ¯ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ç¨®é¡\n")
        content.append("- **å®Œå…¨ä¸€è‡´ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: æ–‡æ›¸ã«å®Œå…¨ã«ä¸€è‡´ã™ã‚‹èªå¥ï¼ˆé«˜ã‚¹ã‚³ã‚¢ï¼‰\n")
        content.append("- **éƒ¨åˆ†ä¸€è‡´ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: æ–‡æ›¸ã«å«ã¾ã‚Œã¦ã„ã‚Œã°OKã®èªå¥ï¼ˆä¸­ã‚¹ã‚³ã‚¢ï¼‰\n")
        content.append("- **é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: ã“ã‚ŒãŒã‚ã‚‹ã¨åˆ¤å®šã‚’é™¤å¤–ã™ã‚‹èªå¥\n")
        content.append("- **ãƒ•ã‚¡ã‚¤ãƒ«åã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰**: ãƒ•ã‚¡ã‚¤ãƒ«åã§ã®ã¿ãƒã‚§ãƒƒã‚¯ã™ã‚‹èªå¥ï¼ˆé‡è¦åº¦é«˜ï¼‰\n")
        content.append("- **ANDæ¡ä»¶**: è¤‡æ•°ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒåŒæ™‚ã«å¿…è¦ãªæ¡ä»¶\n\n")
        
        # ç•ªå°åˆ¥ã®åˆ†é¡
        content.append("## ğŸ“‚ æ›¸é¡åˆ†é¡ä¸€è¦§\n")
        
        # ç•ªå°ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        categories = {
            "0000": {"name": "ğŸ›ï¸ å›½ç¨ç”³å‘Šæ›¸é¡", "items": []},
            "1000": {"name": "ğŸ¢ éƒ½é“åºœçœŒç¨é–¢é€£", "items": []},
            "2000": {"name": "ğŸ¢ å¸‚ç”ºæ‘ç¨é–¢é€£", "items": []},
            "3000": {"name": "ğŸ’° æ¶ˆè²»ç¨é–¢é€£", "items": []},
            "5000": {"name": "ğŸ“Š ä¼šè¨ˆæ›¸é¡", "items": []},
            "6000": {"name": "ğŸ—ï¸ å›ºå®šè³‡ç”£é–¢é€£", "items": []},
            "7000": {"name": "ğŸ“‹ ç¨åŒºåˆ†é–¢é€£", "items": []}
        }
        
        # å„ãƒ«ãƒ¼ãƒ«ã‚’é©åˆ‡ãªã‚«ãƒ†ã‚´ãƒªã«æŒ¯ã‚Šåˆ†ã‘
        for doc_type, rules in self.classification_rules_v5.items():
            # åƒã®ä½ã§åˆ†é¡ï¼ˆ0001 â†’ 0000, 1003 â†’ 1000, 2001 â†’ 2000, ãªã©ï¼‰
            first_digit = doc_type[0]
            category_code = f"{first_digit}000"
            if category_code in categories:
                categories[category_code]["items"].append((doc_type, rules))
        
        # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«Markdownã‚’ç”Ÿæˆ
        for category_code, category_info in categories.items():
            if category_info["items"]:
                content.append(f"### {category_info['name']}\n")
                
                # å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆ
                sorted_items = sorted(category_info["items"], 
                                    key=lambda x: x[1].get("priority", 0), reverse=True)
                
                for doc_type, rules in sorted_items:
                    content.append(self._format_document_type(doc_type, rules))
        
        content.append("\n## ğŸ¤– ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±\n")
        content.append(f"- **ã‚¨ãƒ³ã‚¸ãƒ³ç‰ˆæœ¬**: DocumentClassifierV5\n")
        content.append(f"- **æœ€çµ‚æ›´æ–°**: {datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}\n")
        content.append(f"- **ç·åˆ†é¡æ•°**: {len(self.classification_rules_v5)}ç¨®é¡\n")
        content.append("\n---\n")
        content.append("*ã“ã®è¾æ›¸ã¯ç¨å‹™æ›¸é¡åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ ğŸ“*\n")
        
        return "".join(content)
    
    def _format_document_type(self, doc_type: str, rules: Dict) -> str:
        """å€‹åˆ¥ã®æ›¸é¡ç¨®åˆ¥ã‚’Markdownå½¢å¼ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã™ã‚‹"""
        content = []
        
        # ã‚¿ã‚¤ãƒˆãƒ«
        clean_name = doc_type.replace("_", " ").replace("0001", "").replace("0002", "").replace("0003", "").replace("0004", "")
        priority = rules.get("priority", 0)
        
        # å„ªå…ˆåº¦ã«å¿œã˜ã¦çµµæ–‡å­—ã‚’è¨­å®š
        priority_emoji = "ğŸ”¥" if priority >= 200 else "â­" if priority >= 180 else "ğŸŒŸ" if priority >= 150 else "ğŸ’«"
        
        content.append(f"#### {priority_emoji} {clean_name}\n")
        content.append(f"**å„ªå…ˆåº¦**: {priority} | **åˆ†é¡ã‚³ãƒ¼ãƒ‰**: `{doc_type}`\n\n")
        
        # ANDæ¡ä»¶ï¼ˆæœ€å„ªå…ˆæ¡ä»¶ï¼‰
        highest_priority_conditions = rules.get("highest_priority_conditions", [])
        if highest_priority_conditions:
            content.append("**ğŸ¯ æœ€å„ªå…ˆANDæ¡ä»¶** (ã“ã‚Œã‚‰ãŒã‚ã‚‹ã¨ç¢ºå®Ÿã«åˆ¤å®š)\n")
            for i, condition in enumerate(highest_priority_conditions, 1):
                match_type_text = "ã™ã¹ã¦å¿…è¦" if condition.match_type == "all" else "ã„ãšã‚Œã‹å¿…è¦"
                keywords_text = " + ".join([f"`{kw}`" for kw in condition.keywords])
                content.append(f"{i}. {keywords_text} ({match_type_text})\n")
            content.append("\n")
        
        # å®Œå…¨ä¸€è‡´ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        exact_keywords = rules.get("exact_keywords", [])
        if exact_keywords:
            content.append("**âœ… å®Œå…¨ä¸€è‡´ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰** (é«˜ã‚¹ã‚³ã‚¢)\n")
            for keyword in exact_keywords:
                content.append(f"- `{keyword}`\n")
            content.append("\n")
        
        # éƒ¨åˆ†ä¸€è‡´ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        partial_keywords = rules.get("partial_keywords", [])
        if partial_keywords:
            content.append("**ğŸ” éƒ¨åˆ†ä¸€è‡´ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰** (ä¸­ã‚¹ã‚³ã‚¢)\n")
            for keyword in partial_keywords:
                content.append(f"- `{keyword}`\n")
            content.append("\n")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        filename_keywords = rules.get("filename_keywords", [])
        if filename_keywords:
            content.append("**ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«åå°‚ç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰** (é‡è¦åº¦é«˜)\n")
            for keyword in filename_keywords:
                content.append(f"- `{keyword}`\n")
            content.append("\n")
        
        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        exclude_keywords = rules.get("exclude_keywords", [])
        if exclude_keywords:
            content.append("**âŒ é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰** (ã“ã‚ŒãŒã‚ã‚‹ã¨åˆ¤å®šé™¤å¤–)\n")
            for keyword in exclude_keywords:
                content.append(f"- `{keyword}`\n")
            content.append("\n")
        
        content.append("---\n\n")
        return "".join(content)
    
    def _set_no_split_metadata(self, result: ClassificationResult) -> None:
        """åˆ†é¡çµæœã«no_splitãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¨­å®šï¼ˆè³‡ç”£ãƒ»å¸³ç¥¨ç³»ï¼‰"""
        # è³‡ç”£ãƒ»å¸³ç¥¨ç³»ã®ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        document_code = result.document_type.split('_')[0] if '_' in result.document_type else result.document_type
        
        no_split_codes = {
            "6001", "6002", "6003",  # è³‡ç”£ç³»
            "5001", "5002", "5003", "5004"  # å¸³ç¥¨ç³»
        }
        
        if document_code in no_split_codes:
            result.meta["no_split"] = True
            self._log(f"[meta] no_split=True set for document_type: {result.document_type}")
        else:
            result.meta["no_split"] = False
    
    def _create_classification_result(self, document_type: str, confidence: float, matched_keywords: List[str],
                                    classification_method: str, debug_steps: List[ClassificationStep] = None,
                                    processing_log: List[str] = None, original_doc_type_code: str = None) -> ClassificationResult:
        """ClassificationResultä½œæˆã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆno_splitãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è‡ªå‹•è¨­å®šï¼‰"""
        result = ClassificationResult(
            document_type=document_type,
            confidence=confidence,
            matched_keywords=matched_keywords,
            classification_method=classification_method,
            debug_steps=debug_steps or [],
            processing_log=processing_log or self.processing_log.copy(),
            original_doc_type_code=original_doc_type_code or document_type
        )
        self._set_no_split_metadata(result)
        return result

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨
    classifier = DocumentClassifierV5(debug_mode=True)
    print("æ›¸é¡åˆ†é¡ã‚¨ãƒ³ã‚¸ãƒ³ v5.2 åˆæœŸåŒ–å®Œäº†")
    
    # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
    test_cases = [
        {
            "text": "ãƒ¡ãƒ¼ãƒ«è©³ç´° ç¨®ç›® æ³•äººç¨åŠã³åœ°æ–¹æ³•äººç¨ç”³å‘Šæ›¸ å—ä»˜ç•ªå· 20250731185710521215",
            "filename": "test_houjinzei.pdf",
            "expected": "0003_å—ä¿¡é€šçŸ¥_æ³•äººç¨"
        },
        {
            "text": "ç´ä»˜åŒºåˆ†ç•ªå·é€šçŸ¥ ç¨ç›® æ¶ˆè²»ç¨åŠåœ°æ–¹æ¶ˆè²»ç¨ ç´ä»˜å…ˆ èŠç¨å‹™ç½²",
            "filename": "test_shouhizei.pdf", 
            "expected": "3004_ç´ä»˜æƒ…å ±_æ¶ˆè²»ç¨"
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n=== ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ {i} ===")
        result = classifier.classify_document_v5(test_case["text"], test_case["filename"])
        print(f"çµæœ: {result.document_type}")
        print(f"æœŸå¾…å€¤: {test_case['expected']}")
        print(f"ãƒãƒƒãƒ: {'âœ“' if result.document_type == test_case['expected'] else 'âœ—'}")